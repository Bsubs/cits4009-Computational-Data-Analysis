for(v in colnames(catVars)) {
pii <- paste('pred',v,sep='')
aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])
print(sprintf(
"%s, trainAUC: %4.3f",
pii, aucTrain))
}
for(v in colnames(numVars)) {
pii<-paste('pred',v,sep='')
aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])
if(aucTrain>=0.3) {
print(sprintf(
"%s, trainAUC: %4.3f",
pii,aucTrain))
}
}
ggplot(data=train_data) +
geom_density(aes(x=predcreated_date.log,color=as.factor(average_yearly_earnings.binary)))
logLikelihood <- function(ypred, ytrue) {
sum(ifelse(ytrue, log(ypred), log(1-ypred)), na.rm=T)
}
# Compute the likelihood of the Null model on the train set
logNull <- logLikelihood(sum(train_data[,outcome]==pos)/nrow(train_data), train_data[,outcome]==pos)
cat("The log likelihood of the Null model is:", logNull)
# selCatVars is a vector that keeps the names of the top performing categorical variables.
selCatVars <- c()
minDrop <- 0  # may need to adjust this number
for (v in colnames(catVars)) {
pi <- paste('pred', v, sep='')
devDrop <- 2*(logLikelihood(train_data[,pi], train_data[,outcome]==pos) - logNull)
if (devDrop >= minDrop) {
cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
selCatVars <- c(selCatVars, pi)
}
}
selCatVars
selNumVars <- c()
minDrop <- -10  # may need to adjust this number
for (v in colnames(numVars)) {
pi <- paste('pred', v, sep='')
devDrop <- 2*(logLikelihood(train_data[,pi], train_data[,outcome]==pos) - logNull)
print(devDrop)
if (devDrop >= minDrop) {
cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
selNumVars <- c(selNumVars, pi)
}
}
selNumVars
# This function performs feature selection using forward selection
# @param df The dataframe that contains the features
# @param response The response variable
forward_selection <- function(df, response) {
# Get names of variables minus response variable
predictors <- setdiff(names(df), response)
# What features we select
included <- c()
best_aic <- Inf
# Loop to iteratively add predictors
while(length(predictors) > 0) {
aics <- rep(Inf, length(predictors))
# Test each predictor
for(j in seq_along(predictors)) {
formula <- paste(response, "~", paste(c(included, predictors[j]), collapse = "+"))
model <- lm(formula, data = df)
aics[j] <- AIC(model)
}
# If we find a predictor that reduces the AIC, we include it
if(min(aics) < best_aic) {
best_aic <- min(aics)
include <- predictors[which.min(aics)]
included <- c(included, include)
predictors <- setdiff(predictors, include)
} else {
break
}
}
return(included)
}
clean_data <- function(df){
df1 <- subset(df, select=-c(Country, channel_type, created_month))
df1 <- df1 %>%
rename(
Country = predCountry,
channel_type = predchannel_type,
created_month = predcreated_month
)
cols_to_keep <- !grepl("^pred", names(df))
df1 <- df1[, cols_to_keep]
return(df1)
}
train_data <- clean_data(train_data)
test_data <- clean_data(test_data)
youtube_cleaned <- rbind(train_data, test_data)
selected_features <- forward_selection(train_data, "average_yearly_earnings.binary")
train_forward_sel <- subset(train_data, select=c(selected_features))
test_forward_sel <- subset(test_data, select=c(selected_features))
train_forward_sel$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
test_forward_sel$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)
print(selected_features)
# Perform PCA
pca <- prcomp(subset(train_data, select=-c(average_yearly_earnings.binary)), center = TRUE, scale. = TRUE, rank. = 2)
# Get the explained variance ratio
explained_variance_ratio <- pca$sdev^2 / sum(pca$sdev^2)
# Convert principal components and cn_train_y into a data frame for ggplot
df_pca <- as.data.frame(pca$x)
df_pca$class <- as.factor(train_data$average_yearly_earnings.binary)
# Create a scatter plot with different colors for the two classes using ggplot2
ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
geom_point(aes(shape = class), size = 3) +
scale_color_manual(values = c("coral1", "cornflowerblue"), name = "Class") +
labs(x = sprintf('Principal Component 1 (%.2f%%)', explained_variance_ratio[1]*100),
y = sprintf('Principal Component 2 (%.2f%%)', explained_variance_ratio[2]*100),
title = 'Scatter Plot of Data on First Two Principal Components') +
theme_minimal() +
theme(legend.position = "top")
df_pca <- as.data.frame(pca$x)
df_pca$class <- as.factor(train_data$average_yearly_earnings.binary)
variable_vectors <- pca$rotation[, 1:2]
p <- ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
geom_point(aes(shape = class), size = 3, alpha = 0.5) +   # Semi-transparent points
scale_color_manual(values = c("coral1", "cornflowerblue"), name = "Class") +
theme_minimal() +
theme(legend.position = "top") +
labs(x = 'Principal Component 1',
y = 'Principal Component 2',
title = 'Biplot of Data on First Two Principal Components')
variable_vectors_df <- as.data.frame(variable_vectors)
variable_vectors_df$label <- rownames(variable_vectors)
multiplier <- 2
p <- p +
geom_segment(data = variable_vectors_df,
aes(x = 0, y = 0,
xend = PC1 * 5 * multiplier, yend = PC2 * 5 * multiplier),
arrow = arrow(type = "closed", length = unit(0.15, "inches")),
color = 'black', alpha = 0.5) +
geom_text(data = variable_vectors_df,
aes(x = PC1 * 5.2 * multiplier, y = PC2 * 5.2 * multiplier, label = label),
vjust = -0.5, hjust = 0, alpha = 0.8, inherit.aes = FALSE)
# Display the plot
print(p)
# Compute the PCA without limiting to 2 PC
pca <- prcomp(train_data, center = TRUE, scale. = TRUE)
explained_variance_ratio <- pca$sdev^2 / sum(pca$sdev^2)
cumulative_variance <- cumsum(explained_variance_ratio)
num_components <- which(cumulative_variance >= 0.95)[1]
explained_variance_data <- data.frame(
Dimension = 1:length(explained_variance_ratio),
ExplainedVariance = explained_variance_ratio,
CumulativeVariance = cumulative_variance
)
ggplot(explained_variance_data, aes(x = Dimension)) +
geom_bar(aes(y = ExplainedVariance), stat = "identity", fill = "blue", alpha = 0.7) +
geom_line(aes(y = CumulativeVariance), color = "red", size = 1.2) +
geom_point(aes(y = CumulativeVariance), color = "red") +
geom_vline(aes(xintercept = 10), color = "black", linetype = "dashed", size = 1) +
annotate("text", x = num_components, y = 0.8, label = "Number of components required to retain\n95% of the explained variance", hjust = -0.05) +
labs(
title = "Explained Variance as a Function of Dimensions",
x = "Number of Dimensions",
y = "Variance Explained",
caption = "Blue Bars = Individual Explained Variance\nRed Line = Cumulative Explained Variance"
) +
scale_y_continuous(labels = scales::percent) +
theme_minimal()
# Function to apply PCA transformation on new datasets
transform_data <- function(data, pca, num_components) {
centered_data <- scale(data, center = pca$center, scale = pca$scale)
pc_scores <- centered_data %*% pca$rotation[, 1:num_components]
return(as.data.frame(pc_scores))
}
train_pca <- as.data.frame(pca$x[, 1:num_components])
test_pca <- transform_data(test_data, pca, num_components)
train_pca$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
test_pca$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)
# This function calculates the accuracy, precision, recall and F1 score for a model
cal_scores <- function(train_data, train_pred, calibration_data, calibration_pred){
# Calculate metrics for the training data
train_results <- tibble(
Accuracy  = accuracy_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
Precision = precision_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
Recall    = recall_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
F1        = f_meas_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class)
)
# Calculate metrics for the calibration data
calibration_results <- tibble(
Accuracy  = accuracy_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
Precision = precision_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
Recall    = recall_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
F1        = f_meas_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class)
)
list(train = train_results, calibration = calibration_results)
}
# This function plots precision and recall vs threshold
plot_precision_recall_vs_threshold <- function(pred, data, title) {
# Predict the probabilities
probs <-pred$.pred_1
# Create a data frame with true labels and predicted probabilities
results <- data.frame(
True = data$average_yearly_earnings.binary,
Prob = probs
)
# For each threshold, compute precision and recall
thresholds <- seq(0, 1, by = 0.01)
metrics <- sapply(thresholds, function(thresh) {
predictions <- ifelse(results$Prob > thresh, 1, 0)
# Ensure predictions always have levels 0 and 1
predictions <- factor(predictions, levels = c(0, 1))
precision <- precision_vec(results$True, predictions)
recall <- recall_vec(results$True, predictions)
c(Precision = precision, Recall = recall)
})
# Transform results for plotting
df <- as.data.frame(t(metrics))
df$Threshold <- thresholds
df$Difference <- abs(df$Precision - df$Recall)
intersection_point <- df[which.min(df$Difference), "Threshold"]
p <- ggplot(df, aes(x = Threshold)) +
geom_line(aes(y = Precision, color = 'Precision')) +
geom_line(aes(y = Recall, color = 'Recall')) +
labs(title = title,
y = "Value",
x = "Threshold",
color = "Metric") +
geom_vline(aes(xintercept = 0.5), linetype = "dashed", color = "black", size = 0.5) +
geom_text(aes(x = 0.5, y = 0.2, label = "Default threshold"), angle = 90, vjust = -0.5) +
geom_vline(aes(xintercept = intersection_point), linetype = "dashed", color = "red", size = 0.5) +
geom_text(aes(x = intersection_point, y = 0.2,
label = paste0("Intersection ", round(intersection_point, 2))), angle = 90, vjust = -0.5) +
theme_minimal()
return(p)
}
# This function plots the ROC
plot_roc <- function(predictions, data, title) {
# Predict the probabilities
probs <- predictions$.pred_1
pred <- prediction(probs, data$average_yearly_earnings.binary)
perf <- performance(pred, "tpr", "fpr")
roc_data <- data.frame(
FPR = unlist(perf@x.values),
TPR = unlist(perf@y.values)
)
auc <- performance(pred, measure = "auc")
auc_value <- unlist(auc@y.values)
ggplot(roc_data, aes(x = FPR, y = TPR)) +
geom_line(color = "blue") +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
geom_text(aes(x = 0.5, y = 0.25),
label = paste("AUC =", round(auc_value, 2)),
color = "red") +
labs(title = title,
x = "False Positive Rate",
y = "True Positive Rate") +
coord_fixed(ratio = 1) +
theme_minimal()
}
# Plot confusion matrix
plot_con_matrix <- function(pred, data, title){
cm <- confusionMatrix(as.factor(pred[[1]]), data$average_yearly_earnings.binary)
plt <- as.data.frame(cm$table)
plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
ggplot(plt, aes(Prediction,Reference, fill= Freq)) +
geom_tile() + geom_text(aes(label=Freq)) +
scale_fill_gradient(low="white", high="#009194") +
labs(title = title, x = "Reference",y = "Prediction")
}
# Train a logistic regression model using glm and 10 fold cross validation
train_log_reg <- function(df) {
trControl <- trainControl(method = "cv", number = 10)
model <- train(
average_yearly_earnings.binary ~ .,
data = df,
method = "glm",
family = "binomial",
trControl = trControl
)
return(model)
}
log_reg_forward <- train_log_reg(train_forward_sel)
print(log_reg_forward)
train_pred_logreg <- as_tibble(predict(log_reg_forward, newdata = train_forward_sel, type = "prob")) %>%
mutate(".pred_1" = `1`)
test_pred_logreg <- as_tibble(predict(log_reg_forward, newdata = test_forward_sel, type = "prob")) %>%
mutate(".pred_1" = `1`)
train_class_logreg <- as_tibble(ifelse(train_pred_logreg$.pred_1 > 0.5, 1, 0)) %>%
mutate(".pred_class" = as.factor(value))
test_class_logreg <- as_tibble(ifelse(test_pred_logreg$.pred_1 > 0.5, 1, 0)) %>%
mutate(".pred_class" = as.factor(value))
performance_log_reg_forward_sel <- cal_scores(train_forward_sel, train_class_logreg , test_forward_sel, test_class_logreg)
print(performance_log_reg_forward_sel$train)
print(performance_log_reg_forward_sel$calibration)
p1 <- plot_precision_recall_vs_threshold(train_pred_logreg, train_forward_sel, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_logreg, test_forward_sel, "Precision and Recall vs. Threshold, Test Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_roc(train_pred_logreg, train_forward_sel, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_logreg, test_forward_sel, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_con_matrix(train_class_logreg, train_forward_sel, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_logreg, test_forward_sel, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
log_reg_pca <- train_log_reg(train_pca)
print(log_reg_pca)
train_pred_pca <- as_tibble(predict(log_reg_pca, train_pca, type = "prob")) %>%
mutate(".pred_1" = `1`)
test_pred_pca <- as_tibble(predict(log_reg_pca, test_pca, type = "prob")) %>%
mutate(".pred_1" = `1`)
train_class_pca <- as_tibble(ifelse(train_pred_pca$.pred_1 > 0.5, 1, 0)) %>%
mutate(".pred_class" = as.factor(value))
test_class_pca <- as_tibble(ifelse(test_pred_pca$.pred_1 > 0.5, 1, 0)) %>%
mutate(".pred_class" = as.factor(value))
performance_log_reg_pca <- cal_scores(train_pca, train_class_pca, test_pca, test_class_pca)
print(performance_log_reg_pca$train)
print(performance_log_reg_pca$calibration)
p1 <- plot_precision_recall_vs_threshold(train_pred_pca, train_pca, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_pca , test_pca, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_roc(train_pred_pca, train_pca, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_pca, test_pca, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_con_matrix(train_class_pca, train_pca, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_pca, test_pca, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
train_dt <- function(df) {
trControl <- trainControl(method = "cv", number = 10)
dt_forward_cv <- train(
average_yearly_earnings.binary ~ .,
data = df,
method = "rpart",
trControl = trControl
)
return(dt_forward_cv)
}
dt_forward <- train_dt(train_forward_sel)
print(dt_forward)
train_class_dt <- as_tibble(predict(dt_forward, newdata = train_forward_sel, type = "raw")) %>%
rename(".pred_class" = value)
test_class_dt <- as_tibble(predict(dt_forward, newdata = test_forward_sel, type = "raw")) %>%
rename(".pred_class" = value)
train_pred_dt <- as_tibble(predict(dt_forward, newdata = train_forward_sel, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
test_pred_dt <- as_tibble(predict(dt_forward, newdata = test_forward_sel, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
rpart.plot(dt_forward$finalModel)
performance_dt_forward <- cal_scores(train_forward_sel, train_class_dt, test_forward_sel, test_class_dt)
print(performance_dt_forward$train)
print(performance_dt_forward$calibration)
p1 <- plot_precision_recall_vs_threshold(train_pred_dt, train_forward_sel, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_dt, test_forward_sel, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_roc(train_pred_dt, train_forward_sel, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_dt, test_forward_sel, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_con_matrix(train_class_dt, train_forward_sel, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_dt, test_forward_sel, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
dt_pca <- train_dt(train_pca)
print(dt_pca)
train_class_dt <- as_tibble(predict(dt_pca, newdata = train_pca, type = "raw")) %>%
rename(".pred_class" = value)
test_class_dt <- as_tibble(predict(dt_pca, newdata = test_pca, type = "raw")) %>%
rename(".pred_class" = value)
train_pred_dt <- as_tibble(predict(dt_pca, newdata = train_pca, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
test_pred_dt <- as_tibble(predict(dt_pca, newdata = test_pca, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
rpart.plot(dt_pca$finalModel)
performance_dt_forward <- cal_scores(train_pca, train_class_dt, test_pca, test_class_dt)
print(performance_dt_forward$train)
print(performance_dt_forward$calibration)
p1 <- plot_precision_recall_vs_threshold(train_pred_dt, train_pca, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_dt , test_pca, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_roc(train_pred_dt, train_pca, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_dt, test_pca, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_con_matrix(train_class_dt, train_pca, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_dt, test_pca, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
# sketch
colnames(youtube_cleaned) <- sub("\\.log$", "", colnames(youtube_cleaned))
youtube_clustering <- subset(youtube_cleaned, select=-c(average_yearly_earnings.binary))
youtube_clustering <- transform_cols(youtube_clustering, colnames(youtube_clustering), "norm")
# sketch
youtube_clustering <- youtube_clustering[, grep("\\.norm$", colnames(youtube_clustering))]
colnames(youtube_clustering) <- sub("\\.norm$", "", colnames(youtube_clustering))
d <- dist(youtube_clustering, method="manhattan")
pfit <- hclust(d, method="ward.D2")
plot(pfit, main="Cluster Dendrogram for Youtube Channels", labels=names$Youtuber)
rect.hclust(pfit, k=3)
plot_instances <- function(df, x_col, group_vec) {
# Check if x_col exists in df
if (!x_col %in% colnames(df)) {
stop("The specified x_col does not exist in the dataframe")
}
df$Group <- group_vec
agg_data <- df %>%
group_by(!!sym(x_col), Group) %>%
summarise(Count = n()) %>%
ungroup()
agg_data <- agg_data %>%
arrange(Count) %>%
mutate(!!sym(x_col) := factor(!!sym(x_col), levels = unique(!!sym(x_col))))
# Plotting
p <- ggplot(agg_data, aes_string(y = x_col, x = "Count", fill = "as.factor(Group)")) +
geom_bar(stat = "identity", position = "stack") +
labs(title = paste0("Number of Instances by ", x_col, " and Group"),
x = "Number of Instances",
y = x_col,
fill = "Group") +
theme_minimal() +
theme(axis.text.y = element_text(hjust = 1))
return(p)
}
groups <- cutree(pfit, k=3)
plot_instances(names, "Country", groups)
plot_instances(names, "channel_type", groups)
princ <- prcomp(youtube_clustering)
nComp <- 2
project2D <- as.data.frame(predict(princ, newdata=youtube_clustering)[,1:nComp])
hclust.project2D <- cbind(project2D, cluster=as.factor(groups), country=names$Country)
head(hclust.project2D)
library('grDevices')
find_convex_hull <- function(proj2Ddf, groups) {
do.call(rbind,
lapply(unique(groups),
FUN = function(c) {
f <- subset(proj2Ddf, cluster==c);
f[chull(f),]
}
)
)
}
hclust.hull <- find_convex_hull(hclust.project2D, groups)
ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
geom_point(aes(shape=cluster, color=cluster)) +
geom_text(aes(label=country, color=cluster), hjust=0, vjust=1, size=3) +
geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),
alpha=0.4, linetype=0) + theme(text=element_text(size=8))
kbest.p <- 3
cboot.hclust <- clusterboot(youtube_clustering, clustermethod=hclustCBI,
method="ward.D2", k=kbest.p)
summary(cboot.hclust$result)
groups.cboot <- cboot.hclust$result$partition
values <- 1 - cboot.hclust$bootbrd/100
# Function to return the squared Euclidean distance of two given points x and y
sqr_euDist <- function(x, y) {
sum((x - y)^2)
}
# Function to calculate WSS of a cluster, represented as a n-by-d matrix
# (where n and d are the numbers of rows and columns of the matrix)
# which contains only points of the cluster.
wss <- function(clustermat) {
c0 <- colMeans(clustermat)
sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}
# Function to calculate the total WSS. Argument `scaled_df`: data frame
# with normalised numerical columns. Argument `labels`: vector containing
# the cluster ID (starting at 1) for each row of the data frame.
wss_total <- function(scaled_df, labels) {
wss.sum <- 0
k <- length(unique(labels))
for (i in 1:k)
wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
wss.sum
}
# Function to calculate total sum of squared (TSS) distance of data
# points about the (global) mean. This is the same as WSS when the
# number of clusters (k) is 1.
tss <- function(scaled_df) {
wss(scaled_df)
}
# Function to return the CH indices computed using hierarchical
# clustering (function `hclust`) or k-means clustering (`kmeans`)
# for a vector of k values ranging from 1 to kmax.
CH_index <- function(scaled_df, kmax, method="kmeans") {
if (!(method %in% c("kmeans", "hclust")))
stop("method must be one of c('kmeans', 'hclust')")
npts <- nrow(scaled_df)
wss.value <- numeric(kmax) # create a vector of numeric type
# wss.value[1] stores the WSS value for k=1 (when all the
# data points form 1 large cluster).
wss.value[1] <- wss(scaled_df)
if (method == "kmeans") {
# kmeans
for (k in 2:kmax) {
clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
wss.value[k] <- clustering$tot.withinss
}
} else {
# hclust
d <- dist(scaled_df, method="euclidean")
pfit <- hclust(d, method="ward.D2")
for (k in 2:kmax) {
labels <- cutree(pfit, k=k)
wss.value[k] <- wss_total(scaled_df, labels)
}
}
bss.value <- tss(scaled_df) - wss.value # this is a vector
B <- bss.value / (0:(kmax-1)) # also a vector
W <- wss.value / (npts - 1:kmax) # also a vector
data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
# calculate the CH criterion
crit.df <- CH_index(youtube_clustering, 10, method="hclust")
fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=8))
fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
theme(text=element_text(size=8))
grid.arrange(fig1, fig2, nrow=1)
