df <- df %>%
mutate(across(all_of(features), minmax_scale, .names = "{col}.minmax"))
} else if (trans == "norm") {
# Applying z-normalization
df <- df %>%
mutate(across(all_of(features), scale, .names = "{col}.norm"))
}
return(df)
}
# This function implements a NULL model for a given set of data
# @param features The feature columns
# @param target The target variable
str(youtube)
unique_counts <- sapply(youtube, function(x) length(unique(x)))
unique_counts_df <- data.frame(Column_Name = names(unique_counts), Unique_Count = unique_counts)
print(unique_counts_df)
youtube <- subset(youtube, select = -c(rank, Youtuber, Title))
plot_corr(youtube)
youtube <- subset(youtube, select=-c(`Urban_population`, `country_rank`, `channel_type_rank`))
youtube <- youtube %>%
mutate(video.views = na_if(video.views, 0),
uploads = na_if(uploads, 0))
youtube <- youtube %>%
mutate(created_year = ifelse(created_year < 2005, NA, created_year))
youtube <- youtube %>%
mutate_all(~ ifelse(. %in% c("nan"), NA, .))
youtube <- youtube[!(is.na(youtube$video.views) | is.na(youtube$uploads)), ]
youtube <- youtube %>%
mutate(category = ifelse(is.na(category), "missing", category),
Country = ifelse(is.na(Country), "missing", Country),
Abbreviation = ifelse(is.na(Abbreviation), "missing", Abbreviation),
channel_type = ifelse(is.na(channel_type), "missing", channel_type),
created_month = ifelse(is.na(created_month), "missing", created_month))
youtube <- subset(youtube, select = -c(category, Abbreviation))
plot_hist(youtube, 3, "default")
cols_to_transform <- c(
"subscribers", "video.views", "uploads", "video_views_rank",
"video_views_for_the_last_30_days", "lowest_monthly_earnings",
"highest_monthly_earnings", "lowest_yearly_earnings",
"highest_yearly_earnings", "subscribers_for_last_30_days"
)
youtube <- transform_cols(youtube, colnames(youtube)[sapply(youtube, is.numeric)],"log")
plot_hist(youtube, 3, "log")
youtube <- transform_cols(youtube, cols_to_transform,"minmax")
plot_hist(youtube, 3, "minmax")
youtube <- transform_cols(youtube, cols_to_transform,"norm")
plot_hist(youtube, 3, "norm")
earnings_subset <- youtube[, c("lowest_monthly_earnings",
"highest_monthly_earnings", "lowest_yearly_earnings",
"highest_yearly_earnings", "video_views_for_the_last_30_days", "subscribers_for_last_30_days")]
ggpairs(earnings_subset) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
axis.text.y = element_text(size = 10))
youtube <- youtube %>%
mutate(average_yearly_earnings = (lowest_yearly_earnings + highest_yearly_earnings) / 2)
youtube <- transform_cols(youtube, "average_yearly_earnings", "log")
p1 <- plot_individual_hist(youtube, "average_yearly_earnings", "Average Yearly Earnings", TRUE)
p2 <- plot_individual_hist(youtube, "average_yearly_earnings.log", "Log of Average Yearly Earnings", TRUE)
grid.arrange(p1,p2, ncol=2)
youtube$average_yearly_earnings.binary <- ifelse(youtube$average_yearly_earnings.log > 12.15, 1, 0)
table(youtube$average_yearly_earnings.binary)
youtube$average_yearly_earnings.binary <- ifelse(youtube$average_yearly_earnings.log > 14.14, 1, 0)
table(youtube$average_yearly_earnings.binary)
outcome <- "average_yearly_earnings.binary"
pos <- 1
# Remove highly correlated cols
youtube <- subset(youtube, select=-c(average_yearly_earnings.log, highest_yearly_earnings.log, lowest_yearly_earnings.log, highest_monthly_earnings.log, lowest_monthly_earnings.log, video_views_for_the_last_30_days.log, subscribers_for_last_30_days.log))
# This function splits a dataframe into training and test sets given a particular ratio
# @param df the dataframe to operate on
# @param feature the feature to use as the vector of outcomes
# @param train_ratio the split to use
split_data <- function(df, feature, train_ratio = 0.9) {
train_indices <- createDataPartition(df[,feature],  p= train_ratio, list = FALSE)
# Split the data into training and test sets
train_set <- df[train_indices, ]
test_set <- df[-train_indices, ]
# Return a list containing the training and test sets
list(train = train_set, test = test_set)
}
# Retain only the log transformed variables
youtube_cleaned <- youtube[, grepl("\\.log$", names(youtube))]
youtube_cleaned <- bind_cols(youtube_cleaned, youtube[, c("average_yearly_earnings.binary", "Country", "channel_type", "created_month")])
splits <- split_data(youtube_cleaned, "average_yearly_earnings.binary", train_ratio = 0.9)
train_data <- splits$train
test_data <- splits$test
# This function seperates a dataframe into categorical and numeric variables
# @param df the dataframe to operate on
separate_vars <- function(df) {
numeric_var_names <- names(df)[sapply(df, is.numeric)]
factor_var_names <- names(df)[sapply(df, is.character)]
numVars <- subset(df, select=numeric_var_names)
catVars <- subset(df, select=factor_var_names)
return(list(numVars = numVars, catVars = catVars))
}
sep_train <- separate_vars(train_data)
numVars <- sep_train$numVars
catVars <- sep_train$catVars
mkPredC <- function(outCol, varCol, appCol) {
pPos <- sum(outCol==pos)/length(outCol)
naTab <- table(as.factor(outCol[is.na(varCol)]))
pPosWna <- (naTab/sum(naTab))[pos]
vTab <- table(as.factor(outCol), varCol)
pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3)
pred <- pPosWv[appCol]
pred[is.na(appCol)] <- pPosWna
pred[is.na(pred)] <- pPos
pred
}
for (v in colnames(catVars)) {
pi <- paste('pred', v, sep='')
train_data[,pi] <- mkPredC(train_data[,outcome], train_data[,v], train_data[,v])
test_data[,pi] <- mkPredC(test_data[,outcome], test_data[,v], test_data[,v])
}
# for numerical variables, we convert them into categorical one and
# call the `mkPredC` function above.
mkPredN <- function(outCol, varCol, appCol) {
cuts <- unique(as.numeric(
quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T)))
varC <- cut(varCol, cuts)
appC <- cut(appCol, cuts)
mkPredC(outCol, varC, appC)
}
# now go through all the numerical variables in the `numericVars` vector
# and perform the predictions. Again, the outputs are stored back into
# the data frame.
for (v in colnames(numVars)) {
pi <- paste('pred', v, sep='')
train_data[,pi] <- mkPredN(train_data[,outcome], train_data[,v], train_data[,v])
test_data[,pi] <- mkPredN(test_data[,outcome], test_data[,v], test_data[,v])
}
calcAUC <- function(predcol, outcol) {
perf <- performance(prediction(predcol, outcol==pos),'auc')
as.numeric(perf@y.values)
}
for(v in colnames(catVars)) {
pii <- paste('pred',v,sep='')
aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])
print(sprintf(
"%s, trainAUC: %4.3f",
pii, aucTrain))
}
for(v in colnames(numVars)) {
pii<-paste('pred',v,sep='')
aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])
if(aucTrain>=0.3) {
print(sprintf(
"%s, trainAUC: %4.3f",
pii,aucTrain))
}
}
ggplot(data=train_data) +
geom_density(aes(x=uploads.log,color=as.factor(average_yearly_earnings.binary)))
logLikelihood <- function(ypred, ytrue) {
sum(ifelse(ytrue, log(ypred), log(1-ypred)), na.rm=T)
}
# Compute the likelihood of the Null model on the train set
logNull <- logLikelihood(sum(train_data[,outcome]==pos)/nrow(train_data), train_data[,outcome]==pos)
cat("The log likelihood of the Null model is:", logNull)
# selCatVars is a vector that keeps the names of the top performing categorical variables.
selCatVars <- c()
minDrop <- 0  # may need to adjust this number
for (v in colnames(catVars)) {
pi <- paste('pred', v, sep='')
devDrop <- 2*(logLikelihood(train_data[,pi], train_data[,outcome]==pos) - logNull)
if (devDrop >= minDrop) {
cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
selCatVars <- c(selCatVars, pi)
}
}
selCatVars
selNumVars <- c()
minDrop <- -10  # may need to adjust this number
for (v in colnames(numVars)) {
pi <- paste('pred', v, sep='')
devDrop <- 2*(logLikelihood(train_data[,pi], train_data[,outcome]==pos) - logNull)
print(devDrop)
if (devDrop >= minDrop) {
cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
selNumVars <- c(selNumVars, pi)
}
}
selNumVars
# This function performs feature selection using forward selection
# @param df The dataframe that contains the features
# @param response The response variable
forward_selection <- function(df, response) {
# Get names of variables minus response variable
predictors <- setdiff(names(df), response)
# What features we select
included <- c()
best_aic <- Inf
# Loop to iteratively add predictors
while(length(predictors) > 0) {
aics <- rep(Inf, length(predictors))
# Test each predictor
for(j in seq_along(predictors)) {
formula <- paste(response, "~", paste(c(included, predictors[j]), collapse = "+"))
model <- lm(formula, data = df)
aics[j] <- AIC(model)
}
# If we find a predictor that reduces the AIC, we include it
if(min(aics) < best_aic) {
best_aic <- min(aics)
include <- predictors[which.min(aics)]
included <- c(included, include)
predictors <- setdiff(predictors, include)
} else {
break
}
}
return(included)
}
clean_data <- function(df){
cols_to_keep <- !grepl("^pred", names(train_data))
df1 <- df[, cols_to_keep]
df1 <- subset(df1, select=-c(Country, channel_type, created_month))
if(any(is.na(df1))) {
df1[is.na(df1)] <- 0  # this is sketch af FIX THIS ASAP
}
return(df1)
}
train_clean <- clean_data(train_data)
test_clean <- clean_data(test_data)
selected_features <- forward_selection(train_clean, "average_yearly_earnings.binary")
print(selected_features)
train_forward_sel <- subset(train_clean, select=c(selected_features))
test_forward_sel <- subset(test_clean, select=c(selected_features))
train_forward_sel$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
test_forward_sel$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)
# Perform PCA
pca <- prcomp(subset(train_clean, select=-c(average_yearly_earnings.binary)), center = TRUE, scale. = TRUE, rank. = 2)
# Get the explained variance ratio
explained_variance_ratio <- pca$sdev^2 / sum(pca$sdev^2)
# Convert principal components and cn_train_y into a data frame for ggplot
df_pca <- as.data.frame(pca$x)
df_pca$class <- as.factor(train_clean$average_yearly_earnings.binary)
# Create a scatter plot with different colors for the two classes using ggplot2
ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
geom_point(aes(shape = class), size = 3) +
scale_color_manual(values = c("coral1", "cornflowerblue"), name = "Class") +
labs(x = sprintf('Principal Component 1 (%.2f%%)', explained_variance_ratio[1]*100),
y = sprintf('Principal Component 2 (%.2f%%)', explained_variance_ratio[2]*100),
title = 'Scatter Plot of Data on First Two Principal Components') +
theme_minimal() +
theme(legend.position = "top")
# Convert the principal components and labels into a data frame
df_pca <- as.data.frame(pca$x)
df_pca$class <- as.factor(train_clean$average_yearly_earnings.binary)
# Get the variable vectors (loadings)
variable_vectors <- pca$rotation[, 1:2]
p <- ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
geom_point(aes(shape = class), size = 3, alpha = 0.5) +   # Semi-transparent points
scale_color_manual(values = c("coral1", "cornflowerblue"), name = "Class") +
theme_minimal() +
theme(legend.position = "top") +
labs(x = 'Principal Component 1',
y = 'Principal Component 2',
title = 'Biplot of Data on First Two Principal Components')
variable_vectors_df <- as.data.frame(variable_vectors)
variable_vectors_df$label <- rownames(variable_vectors)
multiplier <- 2
p <- p +
geom_segment(data = variable_vectors_df,
aes(x = 0, y = 0,
xend = PC1 * 5 * multiplier, yend = PC2 * 5 * multiplier),
arrow = arrow(type = "closed", length = unit(0.15, "inches")),
color = 'black', alpha = 0.5) +
geom_text(data = variable_vectors_df,
aes(x = PC1 * 5.2 * multiplier, y = PC2 * 5.2 * multiplier, label = label),
vjust = -0.5, hjust = 0, alpha = 0.8, inherit.aes = FALSE)
# Display the plot
print(p)
# Compute the PCA without limiting to 2 PC
pca <- prcomp(train_clean, center = TRUE, scale. = TRUE)
explained_variance_ratio <- pca$sdev^2 / sum(pca$sdev^2)
cumulative_variance <- cumsum(explained_variance_ratio)
num_components <- which(cumulative_variance >= 0.95)[1]
explained_variance_data <- data.frame(
Dimension = 1:length(explained_variance_ratio),
ExplainedVariance = explained_variance_ratio,
CumulativeVariance = cumulative_variance
)
ggplot(explained_variance_data, aes(x = Dimension)) +
geom_bar(aes(y = ExplainedVariance), stat = "identity", fill = "blue", alpha = 0.7) +
geom_line(aes(y = CumulativeVariance), color = "red", size = 1.2) +
geom_point(aes(y = CumulativeVariance), color = "red") +
geom_vline(aes(xintercept = 10), color = "black", linetype = "dashed", size = 1) +
annotate("text", x = num_components, y = 0.8, label = "Number of components required to retain\n95% of the explained variance", hjust = -0.05) +
labs(
title = "Explained Variance as a Function of Dimensions",
x = "Number of Dimensions",
y = "Variance Explained",
caption = "Blue Bars = Individual Explained Variance\nRed Line = Cumulative Explained Variance"
) +
scale_y_continuous(labels = scales::percent) +
theme_minimal()
# Function to apply PCA transformation on new datasets
transform_data <- function(data, pca, num_components) {
centered_data <- scale(data, center = pca$center, scale = pca$scale)
pc_scores <- centered_data %*% pca$rotation[, 1:num_components]
return(as.data.frame(pc_scores))
}
train_pca <- as.data.frame(pca$x[, 1:num_components])
test_pca <- transform_data(test_clean, pca, num_components)
train_pca$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
test_pca$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)
# This function calculates the accuracy, precision, recall and F1 score for a model
cal_scores <- function(train_data, train_pred, calibration_data, calibration_pred){
# Calculate metrics for the training data
train_results <- tibble(
Accuracy  = accuracy_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
Precision = precision_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
Recall    = recall_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
F1        = f_meas_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class)
)
# Calculate metrics for the calibration data
calibration_results <- tibble(
Accuracy  = accuracy_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
Precision = precision_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
Recall    = recall_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
F1        = f_meas_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class)
)
list(train = train_results, calibration = calibration_results)
}
# This function plots precision and recall vs threshold
plot_precision_recall_vs_threshold <- function(pred, data, title) {
# Predict the probabilities
probs <-pred$.pred_1
# Create a data frame with true labels and predicted probabilities
results <- data.frame(
True = data$average_yearly_earnings.binary,
Prob = probs
)
# For each threshold, compute precision and recall
thresholds <- seq(0, 1, by = 0.01)
metrics <- sapply(thresholds, function(thresh) {
predictions <- ifelse(results$Prob > thresh, 1, 0)
# Ensure predictions always have levels 0 and 1
predictions <- factor(predictions, levels = c(0, 1))
precision <- precision_vec(results$True, predictions)
recall <- recall_vec(results$True, predictions)
c(Precision = precision, Recall = recall)
})
# Transform results for plotting
df <- as.data.frame(t(metrics))
df$Threshold <- thresholds
df$Difference <- abs(df$Precision - df$Recall)
intersection_point <- df[which.min(df$Difference), "Threshold"]
p <- ggplot(df, aes(x = Threshold)) +
geom_line(aes(y = Precision, color = 'Precision')) +
geom_line(aes(y = Recall, color = 'Recall')) +
labs(title = title,
y = "Value",
x = "Threshold",
color = "Metric") +
geom_vline(aes(xintercept = 0.5), linetype = "dashed", color = "black", size = 0.5) +
geom_text(aes(x = 0.5, y = 0.2, label = "Default threshold"), angle = 90, vjust = -0.5) +
geom_vline(aes(xintercept = intersection_point), linetype = "dashed", color = "red", size = 0.5) +
geom_text(aes(x = intersection_point, y = 0.2,
label = paste0("Intersection ", round(intersection_point, 2))), angle = 90, vjust = -0.5) +
theme_minimal()
return(p)
}
# This function plots the ROC
plot_roc <- function(predictions, data) {
# Predict the probabilities
probs <- predictions$.pred_1
pred <- prediction(probs, data$average_yearly_earnings.binary)
perf <- performance(pred, "tpr", "fpr")
roc_data <- data.frame(
FPR = unlist(perf@x.values),
TPR = unlist(perf@y.values)
)
auc <- performance(pred, measure = "auc")
auc_value <- unlist(auc@y.values)
ggplot(roc_data, aes(x = FPR, y = TPR)) +
geom_line(color = "blue") +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
geom_text(aes(x = 0.5, y = 0.25),
label = paste("AUC =", round(auc_value, 2)),
color = "red") +
labs(title = "ROC Curve",
x = "False Positive Rate",
y = "True Positive Rate") +
coord_fixed(ratio = 1) +
theme_minimal()
}
# Plot confusion matrix
plot_con_matrix <- function(pred, data, threshold = 0.5){
cm <- confusionMatrix(as.factor(pred[[1]]), data$average_yearly_earnings.binary)
plt <- as.data.frame(cm$table)
plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
ggplot(plt, aes(Prediction,Reference, fill= Freq)) +
geom_tile() + geom_text(aes(label=Freq)) +
scale_fill_gradient(low="white", high="#009194") +
labs(x = "Reference",y = "Prediction")
}
# Train a logistic regression model using glm and 10 fold cross validation
train_log_reg <- function(df) {
trControl <- trainControl(method = "cv", number = 10)
model <- train(
average_yearly_earnings.binary ~ .,
data = df,
method = "glm",
family = "binomial",
trControl = trControl
)
return(model)
}
log_reg_forward <- train_log_reg(train_forward_sel)
print(log_reg_forward)
train_pred_logreg <- as_tibble(predict(log_reg_forward, newdata = train_forward_sel, type = "prob")) %>%
mutate(".pred_1" = `1`)
test_pred_logreg <- as_tibble(predict(log_reg_forward, newdata = test_forward_sel, type = "prob")) %>%
mutate(".pred_1" = `1`)
train_class_logreg <- as_tibble(ifelse(train_pred_logreg$.pred_1 > 0.5, 1, 0)) %>%
mutate(".pred_class" = as.factor(value))
test_class_logreg <- as_tibble(ifelse(test_pred_logreg$.pred_1 > 0.5, 1, 0)) %>%
mutate(".pred_class" = as.factor(value))
performance_log_reg_forward_sel <- cal_scores(train_forward_sel, train_class_logreg , test_forward_sel, test_class_logreg)
print(performance_log_reg_forward_sel$train)
print(performance_log_reg_forward_sel$calibration)
p1 <- plot_precision_recall_vs_threshold(train_pred_logreg, train_forward_sel, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_logreg, test_forward_sel, "Precision and Recall vs. Threshold, Test Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_roc(train_pred_logreg, train_forward_sel)
p2 <- plot_roc(test_pred_logreg, test_forward_sel)
grid.arrange(p1, p2, ncol=2)
p1 <- plot_con_matrix(train_class_logreg, train_forward_sel)
p2 <- plot_con_matrix(test_class_logreg, test_forward_sel)
grid.arrange(p1, p2, ncol=2)
log_reg_pca <- train_log_reg(train_pca)
print(log_reg_pca)
train_pred_pca <- as_tibble(predict(log_reg_pca, train_pca, type = "prob")) %>%
mutate(".pred_1" = `1`)
test_pred_pca <- as_tibble(predict(log_reg_pca, test_pca, type = "prob")) %>%
mutate(".pred_1" = `1`)
train_class_pca <- as_tibble(ifelse(train_pred_pca$.pred_1 > 0.5, 1, 0)) %>%
mutate(".pred_class" = as.factor(value))
test_class_pca <- as_tibble(ifelse(test_pred_pca$.pred_1 > 0.5, 1, 0)) %>%
mutate(".pred_class" = as.factor(value))
performance_log_reg_pca <- cal_scores(train_pca, train_class_pca, test_pca, test_class_pca)
print(performance_log_reg_pca$train)
print(performance_log_reg_pca$calibration)
p1 <- plot_precision_recall_vs_threshold(train_pred_pca, train_pca, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_pca , test_pca, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_roc(train_pred_pca, train_pca)
p2 <- plot_roc(test_pred_pca, test_pca)
grid.arrange(p1, p2, ncol=2)
p1 <- plot_con_matrix(train_class_pca, train_pca)
p2 <- plot_con_matrix(test_class_pca, test_pca)
grid.arrange(p1, p2, ncol=2)
train_dt <- function(df) {
# Train a decision tree
model <- rpart(formula=average_yearly_earnings.binary ~ ., data = df)
return(model)
}
dt_forward <- train_dt(train_forward_sel)
train_class_dt <- as_tibble(predict(dt_forward, train_forward_sel, type = "class")) %>%
rename(".pred_class" = value)
test_class_dt <- as_tibble(predict(dt_forward, test_forward_sel, type = "class")) %>%
rename(".pred_class" = value)
train_pred_dt <- as_tibble(predict(dt_forward, train_forward_sel, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
test_pred_dt <- as_tibble(predict(dt_forward, test_forward_sel, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
rpart.plot(dt_forward)
performance_dt_forward <- cal_scores(train_forward_sel, train_class_dt, test_forward_sel, test_class_dt)
print(performance_dt_forward$train)
print(performance_dt_forward$calibration)
p1 <- plot_precision_recall_vs_threshold(train_pred_dt, train_forward_sel, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_dt, test_forward_sel, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_roc(train_pred_dt, train_forward_sel)
p2 <- plot_roc(test_pred_dt, test_forward_sel)
grid.arrange(p1, p2, ncol=2)
p1 <- plot_con_matrix(train_class_dt, train_forward_sel)
p2 <- plot_con_matrix(test_class_dt, test_forward_sel)
grid.arrange(p1, p2, ncol=2)
dt_pca <- train_dt(train_pca)
train_class_dt <- as_tibble(predict(dt_pca, train_pca, type = "class")) %>%
rename(".pred_class" = value)
test_class_dt <- as_tibble(predict(dt_pca, test_pca, type = "class")) %>%
rename(".pred_class" = value)
train_pred_dt <- as_tibble(predict(dt_pca, train_pca, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
test_pred_dt <- as_tibble(predict(dt_pca, test_pca, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
rpart.plot(dt_pca)
performance_dt_forward <- cal_scores(train_pca, train_class_dt, test_pca, test_class_dt)
print(performance_dt_forward$train)
print(performance_dt_forward$calibration)
p1 <- plot_precision_recall_vs_threshold(train_pred_dt, train_pca, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_dt , test_pca, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
p1 <- plot_roc(train_pred_dt, train_pca)
p2 <- plot_roc(test_pred_dt, test_pca)
grid.arrange(p1, p2, ncol=2)
p1 <- plot_con_matrix(train_class_dt, train_pca)
p2 <- plot_con_matrix(test_class_dt, test_pca)
grid.arrange(p1, p2, ncol=2)
dt_pca <- train_dt(train_pca)
train_class_dt <- as_tibble(predict(dt_pca, train_pca, type = "class")) %>%
rename(".pred_class" = value)
test_class_dt <- as_tibble(predict(dt_pca, test_pca, type = "class")) %>%
rename(".pred_class" = value)
train_pred_dt <- as_tibble(predict(dt_pca, train_pca, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
test_pred_dt <- as_tibble(predict(dt_pca, test_pca, type = "prob")) %>%
rename(".pred_0" = `0`, ".pred_1" = `1`)
rpart.plot(dt_pca)
