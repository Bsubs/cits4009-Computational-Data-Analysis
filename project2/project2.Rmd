---
title: "CITS4009 - Project 2"
author: "Joo Kai Tay (22489437)"
date: "2023-09-25"
output:
  html_document:
    code_folding: hide
---
```{css}
img {
  display: block;
  margin-bottom: 0 !important;
}
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

- Introduce the youtube dataset and the features

- Start by talking about some EDA stuff 
  - Add graphs and talk about missing data and w/e
  - Histograms
  - Do log transforms of data
  - Explore which variables need to be scaled 
  - Binarize the target variable
    - Try mean / median / other sensible values 
  
- Basic data cleaning
  - Discard character string and categorical columns
  - Convert the factors 
  
- Choose the response variable for the task
  - Formulate it as a binary classification problem 
  
- Split the data into test and train set 
  - Justify this

- Implement a NULL model

- Implement a single-variable model 
  - Try every single variable and pick out the best features
  - Use this as a baseline to compare with the complex models

- Implement decision tree classifier

- Implement the following
  - logistic regression
  - naive bayes 
  - KNN
  - FIND THE MOST INTERESTING ONE AND DO SOMEHING WITH IT
  - Attribute & feature selection 
    - Do a load with this (like a lot, lots and lots)
    - 3-4 attribute selection techniques 
    - Mix and match 
    - and compare the results of all the different types and make pretty graphs for it
    - end myself
  
- Things to investigate
  - Earnings
  - Subscribers
  - 

# Introduction

The 2023 Global YouTube Statistic dataset can be accessed from Kaggle from the link below:
https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023

This dataset contains various information about the top YouTube channels on the platform with the most subscribers. The data set contains information about their subscriber counts, video views, earnings and demographics.

The modelling process explored to preduct **REPLACE ONCE WE KNOW WHAT TO PREDICT** is documented below in terms of code and comments. The plots 

# Data loading, and set up

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(knitr)
library(tidyverse)
library(ggrepel)
library(vtreat)
library(corrplot)
library(tidyr)
library(psych)
library(GGally) 
library(stringr)
library(reshape2)
library(caret)
library(ROCR)
library(MASS)
library(FactoMineR)
library(tidymodels)
library(glmnet)
library(parsnip)
library(yardstick)
```

```{r}
path <- './data/youtube_UTF_8.csv'
youtube <- read.csv(path)
```

# Functions

This section contains all function used in this modelling exercise. 

```{r}
# This function plots a correlation matrix for each numerical value in a given a dataframe
# @param df dataframe to plot
plot_corr <- function(df) {
  youtube_numeric <- df[, sapply(df, is.numeric) | sapply(df, is.integer)]
  
  correlation_matrix <- cor(youtube_numeric, use = "complete.obs")
  correlation_data <- melt(correlation_matrix)
  diag(correlation_matrix) <- NA
  
  format_label <- function(x) {
    ifelse(is.na(x), "", ifelse(abs(x) >= 0.6,sprintf("%.1f", x), ""))
  }
  
  custom_color_gradient <- c('forestgreen', 'lightblue', 'darkblue')
  
  heatmap_plot <- ggplot(correlation_data, aes(Var1, Var2, fill = value)) +
    geom_tile() +
    geom_text(aes(label = format_label(value)), vjust = 0.5, hjust=0.5, size = 2, color='white') +  
    scale_fill_gradientn(colors = custom_color_gradient, values = scales::rescale(c(-1, 0, 1)), guide = "legend") +  
      labs(
      title = "Correlation Matrix",
      x = NULL,
      y = NULL
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      plot.title = element_text(hjust = 0.5))
    
  
  print(heatmap_plot)
}
```

```{r}
# This function plots a histogram for each numerical value in a given dataframe
# @param df dataframe to plot
# @param ncols number of columns in output plot
# @param var.type which selection of variables to plot
plot_hist <- function(df, ncols, var.type){
  
  numeric_cols <- sapply(df, is.numeric)  
  if(var.type == "log"){
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.log$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  } else if(var.type == "default") {
    total_numeric_cols <- sum(numeric_cols) 
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  } else if(var.type == "minmax") {
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.minmax$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))
  } else if(var.type == "norm") {
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.norm$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  }
  
  
  for(colname in names(df)[numeric_cols]) {
    if(var.type == "log" & grepl("\\.log$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if(var.type == "default") {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if (var.type == "minmax" & grepl("\\.minmax$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if(var.type == "norm" & grepl("\\.norm$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    }
  }
  
  # Reset the plotting parameters to default
  par(mfrow=c(1, 1), mar=c(5, 4, 4, 2) + 0.1)
}

```

```{r}
plot_individual_hist <- function(df, feature, title1, plot.mean) {
  mean_val <- mean(df[,feature], na.rm = TRUE)
  median_val <- median(df[,feature], na.rm = TRUE)
  
  p <- ggplot(df, aes(x = df[,feature])) +
  geom_histogram(fill = "lightgreen", alpha = 0.7, color="black") +
  labs(title=title1) +
  xlab(feature) +
  ylab("Frequency") +
  theme_minimal()
  
  if(plot.mean) {
    p <- p +
      geom_vline(aes(xintercept = mean_val), color = "red", linetype = "dashed", size = 1) +
      geom_vline(aes(xintercept = median_val), color = "blue", linetype = "dashed", size = 1) +
      annotate("text", x = mean_val, y = Inf, label = sprintf("Mean = %.2f", mean_val), vjust = 2, hjust = 1.1, color = "red", size = 3.5) +
      annotate("text", x = median_val, y = Inf, label = sprintf("Median = %.2f", median_val), vjust = 2, hjust = -0.1, color = "blue", size = 3.5) 
  }
  
  return(p)
}
```


```{r}
minmax_scale <- function(x) {
  return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
# This function transforms the data 
# @param df the dataframe to transform
# @param features the features to transform
# @param trans the type of transform to apply
transform_cols <- function(df, features, trans) {
  if(trans == "log"){
    # Apply log transform
    df <- df %>%
      mutate(across(all_of(features), ~log(. + 1e-6), .names = "{col}.log"))
  } else if (trans == "minmax") {
    # Applying min-max scaling
    df <- df %>%
      mutate(across(all_of(features), minmax_scale, .names = "{col}.minmax"))
  } else if (trans == "norm") {
    # Applying z-normalization
    df <- df %>%
      mutate(across(all_of(features), scale, .names = "{col}.norm"))
  }
  
  return(df)
}
```

```{r}
# This function implements a NULL model for a given set of data 
# @param features The feature columns
# @param target The target variable
```

# Data Preparation

```{r, results='hide'}
str(youtube)
```
The Global YouTube Statistics 2023 data set contains 995 observations for 28 variables:

- 4 Integer Variables: Integers represent whole numbers without decimal points
- 7 Character Variables: Characters represent text or strings of characters
- 17 Numeric Variables: Represent real numbers, including integers & numbers with decimal points

## Unique Observations 

```{r}
unique_counts <- sapply(youtube, function(x) length(unique(x)))
unique_counts_df <- data.frame(Column_Name = names(unique_counts), Unique_Count = unique_counts)
print(unique_counts_df)
```

The code above counts the number of unique rows in each feature of the data set. It can be observed that the features `rank`, `Youtuber` and `Title` have unique values in each row. When a feature has a unique value for each row, it means there is no discernible pattern or variation in that feature. Machine learning models rely on patterns and relationships in the data to make predictions. Therefore, including features that cannot contribute to learning these patterns would just introduce noise into any model that we attempt to fit to them. For this reason, these features will be dropped from the data set. 

```{r}
youtube <- subset(youtube, select = -c(rank, Youtuber, Title))
```

## Data Correlation Matrix 

```{r,fig.dim = c(15, 15)}
plot_corr(youtube)
```

When investigating if some features can be dropped due to a linear relationship, it is common to plot a scatter matrix that plots each feature against each other feature. However, in this dataset which contains 28 features, that would require 28^2 (784) plots. This is an unrealistic number to plot, display and sort through.

Therefore, we can plot a correlation matrix. The correlation value between a pair of features measures the degree of the linear relationship that they have. If the two features have a high correlation score (approaching 1) it means that they are linear and measure the same variable. Having too many highly correlated variables may present problems in the following ways:

- **Over fitting**: The model may make splits based on a small number of highly correlated features. This may cause over fitting problems.
- **Redundancy**: As seen in section 1.2.1 removing highly correlated features only gives a small drop in accuracy while providing a significant increase in performance. This is because highly correlated features provide very little additional benefits in classification while adding unecessary complexity.
- **Improved Model Performance**: In some cases, removing highly correlated features can actually improve the performance of the model. When features are highly correlated, they can lead to multicollinearity, which can make it difficult for the model to estimate the individual effects of each feature on the target variable.

Examining the plot above, we can observe the following:

- `Urban_population` is highly correlated with `Population`
- `video_views_rank`, `country_rank` and `channel_type_rank` are highly correlated
- `video.views` and `subscribers` are highly correlated

We will drop `Urban_population`, `country_rank` and `channel_type_rank` from the data set.

```{r}
youtube <- subset(youtube, select=-c(`Urban_population`, `country_rank`, `channel_type_rank`))
```

## Data Cleaning

The following steps were discussed in project 1 in relation to the data cleaning and will be repeated here:

There are channels with 0 uploads or video views. We are not interested in these channels as the data is likely to be incorrect given their popularity. Therefore, all observations with video views or uploads equal to 0 will be replaced with NaN.

```{r}
youtube <- youtube %>%
  mutate(video.views = na_if(video.views, 0),
         uploads = na_if(uploads, 0))
```

YouTube was founded in 2005, therefore any observation with a channel creation year prior to 2005 are errors and will be replaced with NaN to be counted as missing values.

```{r}
youtube <- youtube %>%
  mutate(created_year = ifelse(created_year < 2005, NA, created_year))
```

Data points in which the missing data was indicated using `nan` instead of `NA` which is the proper value to represent missing data in R. We will convert all those values to `NaN` in order to have all missing values represented uniformly.

```{r}
youtube <- youtube %>%
  mutate_all(~ ifelse(. %in% c("nan"), NA, .))
```

The rows where `video.views` and `uploads` are missing are also missing most of the data regarding earnings and geographical data. Therefore, we consider these rows to be of low quality and they will be deleted from the data set. 

```{r}
youtube <- youtube[!(is.na(youtube$video.views) | is.na(youtube$uploads)), ]
```

For categorical variables, any values with NA or NaN will be converted to `missing`.

```{r}
youtube <- youtube %>%
  mutate(category = ifelse(is.na(category), "missing", category),
         Country = ifelse(is.na(Country), "missing", Country),
         Abbreviation = ifelse(is.na(Abbreviation), "missing", Abbreviation),
         channel_type = ifelse(is.na(channel_type), "missing", channel_type),
         created_month = ifelse(is.na(created_month), "missing", created_month))
```

The following columns contain duplicate information and will be removed:
```{r}
youtube <- subset(youtube, select = -c(category, Abbreviation))
```

## Histograms of numeric variables

```{r, fig.dim = c(10, 10)}
plot_hist(youtube, 3, "default")
```

The function above shows the plot of histograms for the remaining numeric variables in the data set. It can be observed that many of the features, including `subscribers`, `video.views`, `uploads`, `video_views_rank`, `video_views_for_the_last_30_days`, `lowest_monthly_earnings`, `highest_monthly_earnings`, `lowest_yearly_earnings`, `highest_yearly earnings` and `subscribers_for_last_30_days` are right skewed. This means that the majority of the data points are concentrated on the left side of the distribution, with the tail extending to the right. In such cases, the mean is typically greater than the median, and extreme values can be substantially higher than the rest of the data points.    

Having a right skew can impact model performance as many machine learning models assume that the features have a normal (Gaussian) distribution. When the features are skewed, it can lead to sub optimal performance as the models are less capable of capturing the underlying patterns in the skewed data.

We will investigate several transformations of these variables to determine which is the best option for our data. We will apply a log transform, min-max scaling and z-normalization to the following columns:

```{r}
cols_to_transform <- c(
  "subscribers", "video.views", "uploads", "video_views_rank", 
  "video_views_for_the_last_30_days", "lowest_monthly_earnings", 
  "highest_monthly_earnings", "lowest_yearly_earnings", 
  "highest_yearly_earnings", "subscribers_for_last_30_days"
)
```


### Log transform

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, colnames(youtube)[sapply(youtube, is.numeric)],"log")

plot_hist(youtube, 3, "log")
```

### Min-Max scaling

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, cols_to_transform,"minmax")
plot_hist(youtube, 3, "minmax")
```

The min-max scaler transforms the data to be within a given range which is very useful when dealing with data that has vastly different scales like the YouTube data set. In this case, the range chosen was [0,1]. However, just like z-normalization, min-max scaling is highly sensitive to outliers. Given the highly skewed distribution with a very long tail, min-max scaling will compress the majority of the scaled data into a small interval.

### Z-score normalization

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, cols_to_transform,"norm")
plot_hist(youtube, 3, "norm")
```

Z-normalization involves scaling the features such that they have a mean of zero and a standard deviation of one. However, as shown in the plots above Z-normalization is not very useful for the YouTube data set. This is because Z-normalization is sensitive to outliers. Given that many of the features are heavily skewed with extreme values, the mean and standard deviation can be disproportionately affected. Standardizing in this context may not provide the intended scaling for the majority of the data points. Therefore, using Z-normalization does not address the issue of the skewed data that we are trying to rectify. 

# Target Variable Selection 

Given the nature of the YouTube data set, we will choose earnings as the target variable to determine the success or failure of a YouTube channel. To facilitate the classification, we will formulate it as a binary classification problem (high-earning/low-earning) as multiclass classification is very difficult. 

```{r, fig.width=10, fig.height=4}
earnings_subset <- youtube[, c("lowest_monthly_earnings", 
  "highest_monthly_earnings", "lowest_yearly_earnings", 
  "highest_yearly_earnings")]

ggpairs(earnings_subset) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10), 
        axis.text.y = element_text(size = 10))
```

The pairplot above shows the 4 different features that track the earnings of each YouTube channel. It can be observed that there ia a perfect positive linear relationship in each case and they all have a correlation of 1. It's worth noting that in real-world scenarios, having a correlation of exactly 1 between different variables is very rare, unless there is some deterministic relationship between them or they are different representations of the same underlying data.

We will take an average of the highest and lowest yearly earnings to form a new feature that we will use as our target variable. There are several reasons to use the average of the highest and lowest yearly earnings:

- Predicting the highest earnings may lead to over-optimistic predictions, influenced by extreme values or outliers. An average helps in reducing the influence of such extreme values.
- High earnings may have high variability, and focusing solely on them may lead to instability in predictions. Averaging helps in mitigating this variability.
- A model trained to predict average earnings tends to be more robust, as it learns the broader patterns in the data.

```{r, fig.width=10, fig.height=4}
youtube <- youtube %>%
  mutate(average_yearly_earnings = (lowest_yearly_earnings + highest_yearly_earnings) / 2)

youtube <- transform_cols(youtube, "average_yearly_earnings", "log")

p1 <- plot_individual_hist(youtube, "average_yearly_earnings", "Average Yearly Earnings", TRUE)
p2 <- plot_individual_hist(youtube, "average_yearly_earnings.log", "Log of Average Yearly Earnings", TRUE)

grid.arrange(p1,p2, ncol=2)
```


The histogram on the left shows the average yearly earnings target variable that was just created. Like many of the other features in this data set, it is right skewed. Therefore, we will perform a log transform on it to help normalize the distribution of the target variable. 

As the earnings is presented as a continuous variable, we will need to find a threshold to determine what is considered high and low earning in order to formulate this as a binary classification problem. We will examine using the mean and median as the initial threshold.


```{r}
youtube$average_yearly_earnings.binary <- ifelse(youtube$average_yearly_earnings.log > 12.15, 1, 0)
table(youtube$average_yearly_earnings.binary)
```
```{r}
youtube$average_yearly_earnings.binary <- ifelse(youtube$average_yearly_earnings.log > 14.14, 1, 0)
table(youtube$average_yearly_earnings.binary)
```

The code above shows the number of each class in the newly created binary target variable when using the mean and median as the splitting threshold respectively. We can see that using the median produces a much better balance between the positive and negative classes this is important as the balance between positive and negative classes in the target variable can significantly impact the performance and interpretation of a machine learning model. When there's a strong class imbalance, the majority class can dominate the minority class, leading to poor predictive performance for the minority class. A model may achieve high overall accuracy simply by predicting the majority class all the time, but it's likely that the recall or precision for the minority class will be very low.

Therefore, going forward we will use the median to split the target variable for the binary classification tasks in the remainder of this notebook.

```{r}
outcome <- "average_yearly_earnings.binary"
pos <- 1
```


# Splitting the data

We will use a 90/10 split for the data. Given that the data set is relatively small (1000 obvs), we allocate a larger proportion for training to ensure that the model has enough data to learn for. 

```{r}
# This function splits a dataframe into training and test sets given a particular ratio
# @param df the dataframe to operate on
# @param feature the feature to use as the vector of outcomes
# @param train_ratio the split to use
split_data <- function(df, feature, train_ratio = 0.9) {
  train_indices <- createDataPartition(df[,feature],  p= train_ratio, list = FALSE)
  
  # Split the data into training and test sets
  train_set <- df[train_indices, ]
  test_set <- df[-train_indices, ]
  
  # Return a list containing the training and test sets
  list(train = train_set, test = test_set)
}


```
```{r}
# Retain only the log transformed variables
youtube_cleaned <- youtube[, grepl("\\.log$", names(youtube))]
youtube_cleaned <- bind_cols(youtube_cleaned, youtube[, c("average_yearly_earnings.binary", "Country", "channel_type", "created_month")])

splits <- split_data(youtube_cleaned, "average_yearly_earnings.binary", train_ratio = 0.9)
train_data <- splits$train
test_data <- splits$test

useForCal <- rbinom(n=dim(train_data)[[1]], size=1, prob=0.1) > 0
cal_data <- subset(train_data, useForCal)
train_data <- subset(train_data,!useForCal)
```

Split the data into categorical and numerical variables:

```{r}
# This function seperates a dataframe into categorical and numeric variables
# @param df the dataframe to operate on
separate_vars <- function(df) {
  numeric_var_names <- names(df)[sapply(df, is.numeric)]
  factor_var_names <- names(df)[sapply(df, is.character)]

  numVars <- subset(df, select=numeric_var_names)

  catVars <- subset(df, select=factor_var_names)
  
  return(list(numVars = numVars, catVars = catVars))
}

```

```{r}
sep_train <- separate_vars(train_data)
numVars <- sep_train$numVars
catVars <- sep_train$catVars
```


# Single-Variable Models

The following function is used to process the categorical variables and perform single variable prediction for a given categorical column.

```{r}
mkPredC <- function(outCol, varCol, appCol) {
  pPos <- sum(outCol==pos)/length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab/sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}

for (v in colnames(catVars)) {
  pi <- paste('pred', v, sep='')
  train_data[,pi] <- mkPredC(train_data[,outcome], train_data[,v], train_data[,v])
  cal_data[,pi] <- mkPredC(cal_data[,outcome], cal_data[,v], cal_data[,v])
  test_data[,pi] <- mkPredC(test_data[,outcome], test_data[,v], test_data[,v])
}
```

The following function is used to process the numeric variables and perform single variable prediction for a given numeric column.

```{r}
# for numerical variables, we convert them into categorical one and
# call the `mkPredC` function above.
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(as.numeric(
    quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T)))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}

# now go through all the numerical variables in the `numericVars` vector
# and perform the predictions. Again, the outputs are stored back into
# the data frame.
for (v in colnames(numVars)) {
  pi <- paste('pred', v, sep='')
  train_data[,pi] <- mkPredN(train_data[,outcome], train_data[,v], train_data[,v])
  test_data[,pi] <- mkPredN(test_data[,outcome], test_data[,v], test_data[,v])
  cal_data[,pi] <- mkPredN(cal_data[,outcome], cal_data[,v], cal_data[,v])
}
```

## Evaulating Single Variable Models using AUC

```{r}
calcAUC <- function(predcol, outcol) {
  perf <- performance(prediction(predcol, outcol==pos),'auc')
  as.numeric(perf@y.values)
}
```

```{r, message=TRUE}
for(v in colnames(catVars)) {
  pii <- paste('pred',v,sep='')
  aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])

  aucCal <- calcAUC(cal_data[,pii],cal_data[,outcome])
  print(sprintf(
    "%s, trainAUC: %4.3f calibrationAUC: %4.3f",
    pii, aucTrain, aucCal))
  
}
```
```{r}
for(v in colnames(numVars)) {
  pii<-paste('pred',v,sep='')
  aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])
  
  if(aucTrain>=0.3) {
    aucCal<-calcAUC(cal_data[,pii],cal_data[,outcome])
    print(sprintf(
      "%s, trainAUC: %4.3f calibrationAUC: %4.3f",
      pii,aucTrain,aucCal))
  }
}
```
```{r, fig.height=4}
ggplot(data=cal_data) +
  geom_density(aes(x=uploads.log,color=as.factor(average_yearly_earnings.binary)))
```

## Evaulating Single Variable Models using Log likelihood

```{r}
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1-ypred)), na.rm=T)
}
```

Compute the log likelihood of the Null model

```{r}
# Compute the likelihood of the Null model on the calibration set
logNull <- logLikelihood(sum(cal_data[,outcome]==pos)/nrow(cal_data), cal_data[,outcome]==pos)

cat("The log likelihood of the Null model is:", logNull)
```
Run through categorical variables to select top performers

```{r}
# selCatVars is a vector that keeps the names of the top performing categorical variables.
selCatVars <- c()
minDrop <- 0  # may need to adjust this number

for (v in colnames(catVars)) {
  pi <- paste('pred', v, sep='')
  devDrop <- 2*(logLikelihood(cal_data[,pi], cal_data[,outcome]==pos) - logNull)
  if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selCatVars <- c(selCatVars, pi)
  }
}
selCatVars
```
Run through numerical variables to select top performers

```{r}
selNumVars <- c()
minDrop <- -10  # may need to adjust this number
for (v in colnames(numVars)) {
  pi <- paste('pred', v, sep='')
  devDrop <- 2*(logLikelihood(cal_data[,pi], cal_data[,outcome]==pos) - logNull)
  print(devDrop)
  if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selNumVars <- c(selNumVars, pi)
  }
}
selNumVars
```

# Feature Selection

## Forward Selection

Forward selection using AIC is a stepwise model selection procedure for building regression models. We will choose it as the first method for feature selection.  We begin with the null model and  for each predictor not in the model, we add it to the model and compute the AIC (Akaike Information Criterion). 

AIC is a criterion used for model selection. In the context of multiple regression models, AIC balances the fit of the model (in terms of the likelihood) with the number of predictors used. It is formulated as:

AIC = 2k - 2ln(L)

Where:

- k is the number of parameters in the model.
- L is the likelihood of the model.

The objective in using AIC for model selection is to find the model that has the lowest AIC value. Lower AIC values suggest better model fit when penalizing for the number of predictors, ensuring that we don't overfit with too many variables.

```{r}
# This function performs feature selection using forward selection
# @param df The dataframe that contains the features
# @param response The response variable
forward_selection <- function(df, response) {
  # Get names of variables minus response variable
  predictors <- setdiff(names(df), response)
  # What features we select
  included <- c()
  best_aic <- Inf
  
  # Loop to iteratively add predictors
  while(length(predictors) > 0) {
    aics <- rep(Inf, length(predictors))
    
    # Test each predictor
    for(j in seq_along(predictors)) {
      formula <- paste(response, "~", paste(c(included, predictors[j]), collapse = "+"))
      model <- lm(formula, data = df)
      aics[j] <- AIC(model)
    }
    
    # If we find a predictor that reduces the AIC, we include it
    if(min(aics) < best_aic) {
      best_aic <- min(aics)
      include <- predictors[which.min(aics)]
      included <- c(included, include)
      predictors <- setdiff(predictors, include)
    } else {
      break
    }
  }
  
  return(included)
}
```


```{r}
clean_data <- function(df){
  cols_to_keep <- !grepl("^pred", names(train_data))
  df1 <- df[, cols_to_keep]
  df1 <- subset(df1, select=-c(Country, channel_type, created_month))
  if(any(is.na(df1))) {
    df1[is.na(df1)] <- 0  # this is sketch af FIX THIS ASAP
  }
  return(df1)
}

train_clean <- clean_data(train_data)
test_clean <- clean_data(test_data)
cal_clean <- clean_data(cal_data)


selected_features <- forward_selection(train_clean, "average_yearly_earnings.binary")
print(selected_features)
```

We will create new dataframes for the training, calibration and test sets based on the features from forward selection.

```{r}
train_forward_sel <- subset(train_clean, select=c(selected_features))
cal_forward_sel <- subset(cal_data, select=c(selected_features))
test_forward_sel <- subset(test_data, select=c(selected_features))

train_forward_sel$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
cal_forward_sel$average_yearly_earnings.binary <- as.factor(cal_data$average_yearly_earnings.binary)
test_forward_sel$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)
```


## Principal Component Analysis (PCA)

Many machine learning problems face the problem of having too many features which makes the training process very slow and also can possibly make it hard to converge on an optimal solution, which is referred to as the **curse of dimensionality**. One of the approaches that can be used to solve this is through dimensionality reduction techniques. This not only speeds up training, it also makes it significantly easier to visualize the data, reducing a training set to 2 or 3 dimensions can make it possible to plot a condensed view of a high dimensional training set and gain insights on potential clusters in the data. 

Principal Component Analysis (PCA) is one such dimensionality reduction technique. PCA identifies the hyper plane that preserves the maximum variance of the data and then projects the data onto it. The first principal component is the axis that accounts for the largest amount of the variance in the training set, and the second principal component is orthogonal to the first one, that accounts for the largest amount of the remaining variance.

```{r}
# Perform PCA
pca <- prcomp(subset(train_clean, select=-c(average_yearly_earnings.binary)), center = TRUE, scale. = TRUE, rank. = 2)

# Get the explained variance ratio
explained_variance_ratio <- pca$sdev^2 / sum(pca$sdev^2)

# Convert principal components and cn_train_y into a data frame for ggplot
df_pca <- as.data.frame(pca$x)
df_pca$class <- as.factor(train_clean$average_yearly_earnings.binary)

# Create a scatter plot with different colors for the two classes using ggplot2
ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
  geom_point(aes(shape = class), size = 3) +
  scale_color_manual(values = c("blue", "red"), name = "Class") +
  labs(x = sprintf('Principal Component 1 (%.2f%%)', explained_variance_ratio[1]*100),
       y = sprintf('Principal Component 2 (%.2f%%)', explained_variance_ratio[2]*100),
       title = 'Scatter Plot of Data on First Two Principal Components') +
  theme_minimal() +
  theme(legend.position = "top")
```

The plot above shows the scatter plot of the data on the first two principal components. We can observe the following:

Class Separation: The scatter plot shows that the two classes are linearly separable in the reduced two-dimensional space. This indicates that the first two principal components capture meaningful information that helps distinguish between the classes.

The explained variance ratio provides insight into the relative importance or contribution of each principal component in capturing the underlying structure of the data. Higher values indicate that the corresponding principal component retains more information about the original dataset.

These values indicate that the first principal component explains approximately 43.49% of the total variance in the dataset, while the second principal component explains approximately 13.96% of the total variance. Together, these two components account for a total of approximately 57.45% of the total variance in the data.

The first principal component captures a significant portion of the variance, indicating that it carries valuable information and potentially separates the classes to a certain extent. The second principal component also contributes to the overall variance, but to a lesser extent compared to the first component.


```{r}
# Convert the principal components and labels into a data frame
df_pca <- as.data.frame(pca$x)
df_pca$class <- as.factor(train_clean$average_yearly_earnings.binary)

# Get the variable vectors (loadings)
variable_vectors <- pca$rotation[, 1:2]

p <- ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
  geom_point(aes(shape = class), size = 3, alpha = 0.5) +   # Semi-transparent points
  scale_color_manual(values = c("blue", "red"), name = "Class") +
  theme_minimal() +
  theme(legend.position = "top") +
  labs(x = 'Principal Component 1',
       y = 'Principal Component 2',
       title = 'Biplot of Data on First Two Principal Components')

variable_vectors_df <- as.data.frame(variable_vectors)
variable_vectors_df$label <- rownames(variable_vectors)

multiplier <- 3

p <- p + 
     geom_segment(data = variable_vectors_df, 
                  aes(x = 0, y = 0, 
                      xend = PC1 * 5 * multiplier, yend = PC2 * 5 * multiplier), 
                  arrow = arrow(type = "closed", length = unit(0.15, "inches")), 
                  color = 'black', alpha = 0.5) + 
     geom_text(data = variable_vectors_df, 
               aes(x = PC1 * 5.2 * multiplier, y = PC2 * 5.2 * multiplier, label = label), 
               vjust = -0.5, hjust = 0, alpha = 0.8, inherit.aes = FALSE)

# Display the plot
print(p)
```

The plot above shows the biplot of the data on the first two principal components. The horizontal axis represents the first principal component (PC1), which captures the most significant variation in the data. The vertical axis represents the second principal component (PC2), which captures the second most significant variation orthogonal to PC1.

- The features are represented as arrows in the plot
- The direction and length of the arrows indicate the contribution of each variable to the principal components.
- Variables pointing in the same direction are positively correlated, while those pointing in opposite directions are negatively correlated.
- Longer arrows represent variables with a stronger influence on the principal components.

**Interpreting the plot**:

- At the bottom of the plot, there's a large cluster of highly correlated features for both of the principal components. However, The length of the arrows for each of these features is not very long which suggests that individually, they might not have significant impact on either of the principal components. However, the large number of correlated features could potentially pose an issue.
- In the middle of the plot, we have a long arrow for `concave_points_mean`video_views_rank.log` which is parallel to the axis of principal component 1. This suggests that it is very important to this principal component and holds alot of the variance. 
- At the top of the plot, we have 4 very long arrows for `Gross_tertiary_enrollment.log`, `Unemployment.rate.log`, `Latitude.log` and `Population.log`. These 4 geographic features seem to have a significant weighting in the second principal component.

### Using PCA to retain 95% of the explained variance

```{r, fig.height=4}
# Compute the PCA without limiting to 2 PC
pca <- prcomp(train_clean, center = TRUE, scale. = TRUE)
explained_variance_ratio <- pca$sdev^2 / sum(pca$sdev^2)
cumulative_variance <- cumsum(explained_variance_ratio)
num_components <- which(cumulative_variance >= 0.95)[1]

explained_variance_data <- data.frame(
  Dimension = 1:length(explained_variance_ratio),
  ExplainedVariance = explained_variance_ratio,
  CumulativeVariance = cumulative_variance
)


ggplot(explained_variance_data, aes(x = Dimension)) +
  geom_bar(aes(y = ExplainedVariance), stat = "identity", fill = "blue", alpha = 0.7) +
  geom_line(aes(y = CumulativeVariance), color = "red", size = 1.2) +
  geom_point(aes(y = CumulativeVariance), color = "red") +
  geom_vline(aes(xintercept = 10), color = "black", linetype = "dashed", size = 1) +
  annotate("text", x = num_components, y = 0.8, label = "Number of components required to retain\n95% of the explained variance", hjust = -0.05) +
  labs(
    title = "Explained Variance as a Function of Dimensions",
    x = "Number of Dimensions",
    y = "Variance Explained",
    caption = "Blue Bars = Individual Explained Variance\nRed Line = Cumulative Explained Variance"
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()

```


The explained variance ratio indicates the proportion of the dataset’s variance that lies along each principal component. As seen in the first plot, the first 2 principal components capture only 57.45% of the variance in the dataset. This is not sufficient information as we are losing more than 40% of the explained variance. In order for a more accurate prediction, we would want to retain at least 95% of the explained variance. 

The plot above shows the explained variance as a function of number of dimensions. The line in red shows the cumulative explained variance and the bars in blue shows the individual explained variance of each principal component. It can be observed that after 10 dimensions, we have captured 95% of the explained variance in the data and adding any subsequent dimensions provides very little for the increasing complexity. 

```{r}
# Function to apply PCA transformation on new datasets
transform_data <- function(data, pca, num_components) {
  centered_data <- scale(data, center = pca$center, scale = pca$scale)
  pc_scores <- centered_data %*% pca$rotation[, 1:num_components]
  return(as.data.frame(pc_scores))
}


train_pca <- as.data.frame(pca$x[, 1:num_components])
cal_pca <- transform_data(cal_clean, pca, num_components)
test_pca <- transform_data(test_clean, pca, num_components)

train_pca$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
cal_pca$average_yearly_earnings.binary <- as.factor(cal_data$average_yearly_earnings.binary)
test_pca$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)
```


# Evaluation of Models


## Functions 

**DESCRIBE EACH FUNCTION (purpose)**

```{r}
# This function calculates the accuracy, precision, recall and F1 score for a model
cal_scores <- function(model, train_data, calibration_data){
  # Predict outcomes for the training data
  train_pred <- predict(model, train_data, type = "class")
  
  # Predict outcomes for the calibration data
  calibration_pred <- predict(model, calibration_data, type = "class")
  
  # Calculate metrics for the training data
  train_results <- tibble(
    Accuracy  = accuracy_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
    Precision = precision_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
    Recall    = recall_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
    F1        = f_meas_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class)
  )
  
  # Calculate metrics for the calibration data
  calibration_results <- tibble(
    Accuracy  = accuracy_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
    Precision = precision_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
    Recall    = recall_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
    F1        = f_meas_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class)
  )
  
  list(train = train_results, calibration = calibration_results)
}
```

```{r}
# This function plots precision and recall vs threshold
plot_precision_recall_vs_threshold <- function(model, data, title) {
  # Predict the probabilities
  probs <- predict(model, data, type = "prob")$.pred_1
  
  # Create a data frame with true labels and predicted probabilities
  results <- data.frame(
    True = data$average_yearly_earnings.binary,
    Prob = probs
  )
  
  # For each threshold, compute precision and recall
  thresholds <- seq(0, 1, by = 0.01)
  metrics <- sapply(thresholds, function(thresh) {
    predictions <- ifelse(results$Prob > thresh, 1, 0)
    # Ensure predictions always have levels 0 and 1
    predictions <- factor(predictions, levels = c(0, 1))
    
    precision <- precision_vec(results$True, predictions)
    recall <- recall_vec(results$True, predictions)
    c(Precision = precision, Recall = recall)
  })
  
  # Transform results for plotting
  df <- as.data.frame(t(metrics))
  df$Threshold <- thresholds
  df$Difference <- abs(df$Precision - df$Recall)
  intersection_point <- df[which.min(df$Difference), "Threshold"]
  
  p <- ggplot(df, aes(x = Threshold)) +
    geom_line(aes(y = Precision, color = 'Precision')) +
    geom_line(aes(y = Recall, color = 'Recall')) +
    labs(title = title, 
         y = "Value", 
         x = "Threshold",
         color = "Metric") +
    geom_vline(aes(xintercept = 0.5), linetype = "dashed", color = "black", size = 0.5) +
    geom_text(aes(x = 0.5, y = 0.2, label = "Default threshold"), angle = 90, vjust = -0.5) +
    geom_vline(aes(xintercept = intersection_point), linetype = "dashed", color = "red", size = 0.5) +
    geom_text(aes(x = intersection_point, y = 0.2, 
                  label = paste0("Intersection ", round(intersection_point, 2))), angle = 90, vjust = -0.5) +
    theme_minimal()
  
  return(p)
}
```


```{r}
# This function plots the ROC 
plot_roc <- function(model, data) {
  # Predict the probabilities
  probs <- predict(model, data, type = "prob")$.pred_1
  
  # Create a data frame with true labels and predicted probabilities
  results <- data.frame(
    True = data$average_yearly_earnings.binary,
    Prob = probs
  )
  
  # Compute the ROC curve values using yardstick::roc_curve()
  roc_values <- roc_curve(results, True, Prob)
  
  # Compute the AUC
  auc_value <- roc_auc(results, True, Prob)
  
  # Plot the ROC curve
  p <- ggplot(roc_values, aes(x = 1 - specificity, y = sensitivity)) +
    geom_line() +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    labs(title = paste("ROC Curve (AUC =", round(auc_value$.estimate, 3), ")"), 
         x = "False Positive Rate", 
         y = "True Positive Rate") +
    theme_minimal()
  
  return(p)
}
```

```{r}
# Plot confusion matrix
plot_con_matrix <- function(model, data, threshold = 0.5){
  train_pred <- predict(model, data, type = "class")
  cm <- confusionMatrix(as.factor(train_pred[[1]]), data$average_yearly_earnings.binary)
  plt <- as.data.frame(cm$table)
  plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
  ggplot(plt, aes(Prediction,Reference, fill= Freq)) +
          geom_tile() + geom_text(aes(label=Freq)) +
          scale_fill_gradient(low="white", high="#009194") +
          labs(x = "Reference",y = "Prediction") 
}
```

# Logistic Regression Classifier

Logistic regression aims to find the best possible fit between the selected features and the target variable to predict the probability of the target variable being of high or low earnings. 

The following function returns a logistic regression model given a set of inputs, we will use this to test the various inputs given the two feature selection strategies, forward selection and PCA. The model uses lasso regression and the glmnet engine which allows us to set a penalty value. The strength of this penalty value can be tuned to give us optimal results. 

```{r}
train_log_reg <- function(df) {
  # Train a logistic regression model
  model <- logistic_reg(mixture = double(1), penalty = double(1)) %>%
    set_engine("glmnet") %>%
    set_mode("classification") %>%
    fit(average_yearly_earnings.binary ~ ., data = df)
  
  return(model)
}
```

## Logistic Regression with Forward Selection Features

```{r}
log_reg_forward <- train_log_reg(train_forward_sel)
tidy(log_reg_forward)
```

```{r}
performance_log_reg_forward_sel <- cal_scores(log_reg_forward, train_forward_sel, cal_forward_sel)
print(performance_log_reg_forward_sel$train)
print(performance_log_reg_forward_sel$calibration)
```


```{r, fig.width=10, fig.height=4}
p1 <- plot_precision_recall_vs_threshold(log_reg_forward, train_forward_sel, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(log_reg_forward, cal_forward_sel, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_roc(log_reg_forward, train_forward_sel)
p2 <- plot_roc(log_reg_forward, cal_forward_sel)
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_con_matrix(log_reg_forward, train_forward_sel)
p2 <- plot_con_matrix(log_reg_forward, cal_forward_sel)
grid.arrange(p1, p2, ncol=2)
```


## Logistic Regression Classifier with PCA Features

```{r}
log_reg_pca <- train_log_reg(train_pca)
tidy(log_reg_pca)
```

```{r}
performance_log_reg_pca <- cal_scores(log_reg_pca, train_pca, cal_pca)
print(performance_log_reg_pca$train)
print(performance_log_reg_pca$calibration)
```

```{r, fig.width=10, fig.height=4}
p1 <- plot_precision_recall_vs_threshold(log_reg_pca, train_pca, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(log_reg_pca, cal_pca, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_roc(log_reg_pca, train_pca)
p2 <- plot_roc(log_reg_pca, cal_pca)
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_con_matrix(log_reg_pca, train_pca)
p2 <- plot_con_matrix(log_reg_pca, cal_pca)
grid.arrange(p1, p2, ncol=2)
```





