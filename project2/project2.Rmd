---
title: "CITS4009 - Project 2"
author: "Joo Kai Tay (22489437)"
date: "2023-09-25"
output:
  html_document:
    code_folding: hide
---
```{css}
img {
  display: block;
  margin-bottom: 0 !important;
}
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

The 2023 Global YouTube Statistic dataset can be accessed from Kaggle from the link below:
https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023

This dataset contains various information about the top YouTube channels on the platform with the most subscribers. The data set contains information about their subscriber counts, video views, earnings and demographics.

The modelling process explored to predict high/low earnings is documented below in terms of code and comments.  

# Data loading, and set up

Load the necessary libraries for data preparation, classification and clustering.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(knitr)
library(tidyverse)
library(ggrepel)
library(vtreat)
library(corrplot)
library(tidyr)
library(psych)
library(GGally) 
library(stringr)
library(reshape2)
library(caret)
library(ROCR)
library(MASS)
library(FactoMineR)
library(tidymodels)
library(glmnet)
library(parsnip)
library(yardstick)
library(rpart)
library(rpart.plot)
library(fpc)
```

Read the YouTube dataset.

```{r}
path <- './data/youtube_UTF_8.csv'
youtube <- read.csv(path)
```

# Data Preparation

## Functions

This section contains all function used in the data preparation phase for this modelling exercise.

This function plots a correlation matrix for each numerical value in a given a dataframe. In addition to the color gradient, The function will display any correlation above 0.6 on the matrix to highlight to the user where the highly correlated values are in this dataset.

```{r}
# This function plots a correlation matrix for each numerical value in a given a dataframe
# @param df dataframe to plot
plot_corr <- function(df) {
  youtube_numeric <- df[, sapply(df, is.numeric) | sapply(df, is.integer)]
  
  correlation_matrix <- cor(youtube_numeric, use = "complete.obs")
  correlation_data <- melt(correlation_matrix)
  diag(correlation_matrix) <- NA
  
  format_label <- function(x) {
    ifelse(is.na(x), "", ifelse(abs(x) >= 0.6,sprintf("%.1f", x), ""))
  }
  
  custom_color_gradient <- c('forestgreen', 'lightblue', 'darkblue')
  
  heatmap_plot <- ggplot(correlation_data, aes(Var1, Var2, fill = value)) +
    geom_tile() +
    geom_text(aes(label = format_label(value)), vjust = 0.5, hjust=0.5, size = 2, color='white') +  
    scale_fill_gradientn(colors = custom_color_gradient, values = scales::rescale(c(-1, 0, 1)), guide = "legend") +  
      labs(
      title = "Correlation Matrix",
      x = NULL,
      y = NULL
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      plot.title = element_text(hjust = 0.5))
    
  
  print(heatmap_plot)
}
```

This function plots a histogram for each numerical value in a given dataframe. The function also allows us to select what type of variables we want to see which is very useful when exploring different feature combinations and attribute scaling methods. 

```{r}
# This function plots a histogram for each numerical value in a given dataframe
# @param df dataframe to plot
# @param ncols number of columns in output plot
# @param var.type which selection of variables to plot
plot_hist <- function(df, ncols, var.type){
  
  numeric_cols <- sapply(df, is.numeric)  
  if(var.type == "log"){
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.log$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  } else if(var.type == "default") {
    total_numeric_cols <- sum(numeric_cols) 
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  } else if(var.type == "minmax") {
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.minmax$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))
  } else if(var.type == "norm") {
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.norm$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  }
  
  
  for(colname in names(df)[numeric_cols]) {
    if(var.type == "log" & grepl("\\.log$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if(var.type == "default") {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if (var.type == "minmax" & grepl("\\.minmax$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if(var.type == "norm" & grepl("\\.norm$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    }
  }
  
  # Reset the plotting parameters to default
  par(mfrow=c(1, 1), mar=c(5, 4, 4, 2) + 0.1)
}

```

This function plots the histogram of an given variable and also shows the location of the mean and median.

```{r}
plot_individual_hist <- function(df, feature, title1, plot.mean) {
  mean_val <- mean(df[,feature], na.rm = TRUE)
  median_val <- median(df[,feature], na.rm = TRUE)
  
  p <- ggplot(df, aes(x = df[,feature])) +
  geom_histogram(fill = "lightgreen", alpha = 0.7, color="black") +
  labs(title=title1) +
  xlab(feature) +
  ylab("Frequency") +
  theme_minimal()
  
  if(plot.mean) {
    p <- p +
      geom_vline(aes(xintercept = mean_val), color = "red", linetype = "dashed", size = 1) +
      geom_vline(aes(xintercept = median_val), color = "blue", linetype = "dashed", size = 1) +
      annotate("text", x = mean_val, y = Inf, label = sprintf("Mean = %.2f", mean_val), vjust = 3, hjust = 3, color = "red", size = 3.5) +
      annotate("text", x = median_val, y = Inf, label = sprintf("Median = %.2f", median_val), vjust = 5, hjust = 3, color = "blue", size = 3.5) 
  }
  
  return(p)
}
```

```{r, echo=FALSE}
minmax_scale <- function(x) {
  return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
```

This function performs feature transformation on a given dataframe. We have the option to choose between log transformation, min-max scaling and z-normalization. 

```{r}
# This function transforms the data 
# @param df the dataframe to transform
# @param features the features to transform
# @param trans the type of transform to apply
transform_cols <- function(df, features, trans) {
  if(trans == "log"){
    # Apply log transform
    df <- df %>%
      mutate(across(all_of(features), ~log(. + 1e-6), .names = "{col}.log"))
  } else if (trans == "minmax") {
    # Applying min-max scaling
    df <- df %>%
      mutate(across(all_of(features), minmax_scale, .names = "{col}.minmax"))
  } else if (trans == "norm") {
    # Applying z-normalization
    df <- df %>%
      mutate(across(all_of(features), ~as.vector(scale(.)), .names = "{col}.norm"))
  }
  
  return(df)
}
```

## Inspecting the data

```{r, results='hide'}
str(youtube)
```
The Global YouTube Statistics 2023 data set contains 995 observations for 28 variables:

- 4 Integer Variables: Integers represent whole numbers without decimal points
- 7 Character Variables: Characters represent text or strings of characters
- 17 Numeric Variables: Represent real numbers, including integers & numbers with decimal points


## Data Correlation Matrix 

```{r,fig.dim = c(9, 8)}
plot_corr(youtube)
```

When investigating if some features can be dropped due to a linear relationship, it is common to plot a scatter matrix that plots each feature against each other feature. However, in this dataset which contains 28 features, that would require 28^2 (784) plots. This is an unrealistic number to plot, display and sort through.

Therefore, we can plot a correlation matrix. The correlation value between a pair of features measures the degree of the linear relationship that they have. If the two features have a high correlation score (approaching 1) it means that they are linear and measure the same variable. Having too many highly correlated variables may present problems in the following ways:

- **Over fitting**: The model may make splits based on a small number of highly correlated features. This may cause over fitting problems.
- **Redundancy**: Removing highly correlated features only gives a small drop in accuracy while providing a significant increase in performance. This is because highly correlated features provide very little additional benefits in classification while adding unecessary complexity.
- **Improved Model Performance**: In some cases, removing highly correlated features can actually improve the performance of the model. When features are highly correlated, they can lead to multicollinearity, which can make it difficult for the model to estimate the individual effects of each feature on the target variable.

Examining the plot above, we can observe the following:

- `Urban_population` is highly correlated with `Population`
- `video_views_rank`, `country_rank` and `channel_type_rank` are highly correlated
- `video.views` and `subscribers` are highly correlated
- `video_views_for_the_last_30_days`, `lowest_monthly_earnings`, `highest_monthly_earnings`, `lowest_yearly_earnings` and `highest_yearly_earnings`
- `subscribers_for_last_30_days`, `lowest_monthly_earnings`, `highest_monthly_earnings`, `lowest_yearly_earnings` and `highest_yearly_earnings`

We will drop `Urban_population`, `country_rank` and `channel_type_rank` from the data set. As they are highly correlated with other variables. However, we will leave dealing with `video_views_for_the_last_30_days` and `subscribers_for_last_30_days` as they are highly correlated with our target variable and need to be examined in greater detail. 

```{r}
youtube <- subset(youtube, select=-c(`Urban_population`, `country_rank`, `channel_type_rank`))
```

## Data Cleaning

The following steps were discussed in project 1 in relation to the data cleaning and will be repeated here:

There are channels with 0 uploads or video views. We are not interested in these channels as the data is likely to be incorrect given their popularity. Therefore, all observations with video views or uploads equal to 0 will be replaced with NaN.

```{r}
youtube <- youtube %>%
  mutate(video.views = na_if(video.views, 0),
         uploads = na_if(uploads, 0))
```

YouTube was founded in 2005, therefore any observation with a channel creation year prior to 2005 are errors and will be replaced with NaN to be counted as missing values.

```{r}
youtube <- youtube %>%
  mutate(created_year = ifelse(created_year < 2005, NA, created_year))
```

Data points in which the missing data was indicated using `nan` instead of `NA` which is the proper value to represent missing data in R. We will convert all those values to `NaN` in order to have all missing values represented uniformly.

```{r}
youtube <- youtube %>%
  mutate_all(~ ifelse(. %in% c("nan"), NA, .))
```

The rows where `video.views` and `uploads` are missing are also missing most of the data regarding earnings and geographical data. Therefore, we consider these rows to be of low quality and they will be deleted from the data set. 

```{r}
youtube <- youtube[!(is.na(youtube$video.views) | is.na(youtube$uploads)), ]
```

For categorical variables, any values with NA or NaN will be converted to `missing`.

```{r}
youtube <- youtube %>%
  mutate(category = ifelse(is.na(category), "missing", category),
         Country = ifelse(is.na(Country), "missing", Country),
         Abbreviation = ifelse(is.na(Abbreviation), "missing", Abbreviation),
         channel_type = ifelse(is.na(channel_type), "missing", channel_type),
         created_month = ifelse(is.na(created_month), "missing", created_month))
```

The following columns contain duplicate information and will be removed:
```{r}
youtube <- subset(youtube, select = -c(category, Abbreviation))
```

## Unique Observations 

```{r, results="hide"}
unique_counts <- sapply(youtube, function(x) length(unique(x)))
unique_counts_df <- data.frame(Column_Name = names(unique_counts), Unique_Count = unique_counts)
print(unique_counts_df)
```

The code above counts the number of unique rows in each feature of the data set. It can be observed that the features `rank`, `Youtuber` and `Title` have unique values in each row. When a feature has a unique value for each row, it means there is no discernible pattern or variation in that feature. Machine learning models rely on patterns and relationships in the data to make predictions. Therefore, including features that cannot contribute to learning these patterns would just introduce noise into any model that we attempt to fit to them. For this reason, these features will be dropped from the data set. 

```{r, echo=FALSE}
names <- subset(youtube, select = c(rank, Youtuber, Title, Country, channel_type))
```


```{r}
youtube <- subset(youtube, select = -c(rank, Youtuber, Title))
```

## Histograms of numeric variables

```{r, fig.dim = c(10, 10)}
plot_hist(youtube, 3, "default")
```

The function above shows the plot of histograms for the remaining numeric variables in the data set. It can be observed that many of the features, including `subscribers`, `video.views`, `uploads`, `video_views_rank`, `video_views_for_the_last_30_days`, `lowest_monthly_earnings`, `highest_monthly_earnings`, `lowest_yearly_earnings`, `highest_yearly earnings` and `subscribers_for_last_30_days` are right skewed. This means that the majority of the data points are concentrated on the left side of the distribution, with the tail extending to the right. In such cases, the mean is typically greater than the median, and extreme values can be substantially higher than the rest of the data points.    

Having a right skew can impact model performance as many machine learning models assume that the features have a normal (Gaussian) distribution. When the features are skewed, it can lead to sub optimal performance as the models are less capable of capturing the underlying patterns in the skewed data.

We will investigate several transformations of these variables to determine which is the best option for our data. We will apply a log transform, min-max scaling and z-normalization to the following columns:

```{r}
cols_to_transform <- c(
  "subscribers", "video.views", "uploads", "video_views_rank", 
  "video_views_for_the_last_30_days", "lowest_monthly_earnings", 
  "highest_monthly_earnings", "lowest_yearly_earnings", 
  "highest_yearly_earnings", "subscribers_for_last_30_days"
)
```


### Log transform

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, colnames(youtube)[sapply(youtube, is.numeric)],"log")

plot_hist(youtube, 3, "log")
```

A log transform is the most effect method for dealing with the right skewed data. Many statistical techniques and machine learning algorithms assume that data follows a normal (or Gaussian) distribution. Right-skewed data doesn't meet this assumption. Applying a log transformation can help in making the data look more like a normal distribution. However, from our examinations of the data in the earlier parts of this document, we understand that many variables do have 0s in them and the logarithm of zero and negative numbers is undefined in the real number system. Therefore, the scaling function adds a small constant of `1e-6` to these values to avoid any infinite values in the transformed dataset. 

### Min-Max scaling

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, cols_to_transform,"minmax")
plot_hist(youtube, 3, "minmax")
```

The min-max scaler transforms the data to be within a given range which is very useful when dealing with data that has vastly different scales like the YouTube data set. In this case, the range chosen was [0,1]. However, just like z-normalization, min-max scaling is highly sensitive to outliers. Given the highly skewed distribution with a very long tail, min-max scaling will compress the majority of the scaled data into a small interval which is not helpful when trying to deal with a skewed distribution. 

### Z-score normalization

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, cols_to_transform,"norm")
plot_hist(youtube, 3, "norm")
```

Z-normalization involves scaling the features such that they have a mean of zero and a standard deviation of one. However, as shown in the plots above Z-normalization is not very useful for the YouTube data set. This is because Z-normalization is sensitive to outliers. Given that many of the features are heavily skewed with extreme values, the mean and standard deviation can be disproportionately affected. Standardizing in this context may not provide the intended scaling for the majority of the data points. Therefore, using Z-normalization does not address the issue of the skewed data that we are trying to rectify. 

# Target Variable Selection 

Given the nature of the YouTube data set, we will choose earnings as the target variable to determine the success or failure of a YouTube channel. To facilitate the classification, we will formulate it as a binary classification problem (high-earning/low-earning) as multiclass classification is very difficult. 

```{r, fig.width=10, fig.height=6}
earnings_subset <- youtube[, c("lowest_monthly_earnings", 
  "highest_monthly_earnings", "lowest_yearly_earnings", 
  "highest_yearly_earnings", "video_views_for_the_last_30_days", "subscribers_for_last_30_days")]

ggpairs(earnings_subset) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10), 
        axis.text.y = element_text(size = 10))
```

The pairplot above shows the 4 different features that track the earnings of each YouTube channel as well as `video_views_for_the_last_30_days` and `subscribers_for_last_30_days`. It can be observed that there is a perfect positive linear relationship in each case and the earnings features have a correlation of 1. It's worth noting that in real-world scenarios, having a correlation of exactly 1 between different variables is very rare, unless there is some deterministic relationship between them or they are different representations of the same underlying data. In this case, one possibility is that these variables were calculated from each other which is what gives us that perfect correlation.

We will take an average of the highest and lowest yearly earnings to form a new feature that we will use as our target variable. There are several reasons to use the average of the highest and lowest yearly earnings:

- Predicting the highest earnings may lead to over-optimistic predictions, influenced by extreme values or outliers. An average helps in reducing the influence of such extreme values.
- High earnings may have high variability, and focusing solely on them may lead to instability in predictions. Averaging helps in mitigating this variability.
- A model trained to predict average earnings tends to be more robust, as it learns the broader patterns in the data.

Once this target variable is created, we will remove the original 6 highly correlated variables to prevent them from affecting the training of the model. 

```{r, fig.width=10, fig.height=4}
youtube <- youtube %>%
  mutate(average_yearly_earnings = (lowest_yearly_earnings + highest_yearly_earnings) / 2)

youtube <- transform_cols(youtube, "average_yearly_earnings", "log")

p1 <- plot_individual_hist(youtube, "average_yearly_earnings", "Average Yearly Earnings", TRUE)
p2 <- plot_individual_hist(youtube, "average_yearly_earnings.log", "Log of Average Yearly Earnings", TRUE)

grid.arrange(p1,p2, ncol=2)
```


The histogram on the left shows the average yearly earnings target variable that was just created. Like many of the other features in this data set, it is right skewed. Therefore, we will perform a log transform on it to help normalize the distribution of the target variable. 

As the earnings is presented as a continuous variable, we will need to find a threshold to determine what is considered high and low earning in order to formulate this as a binary classification problem. We will examine using the mean and median as the initial threshold.


```{r}
youtube$average_yearly_earnings.binary <- ifelse(youtube$average_yearly_earnings.log > 12.15, 1, 0)
table(youtube$average_yearly_earnings.binary)
```
```{r}
youtube$average_yearly_earnings.binary <- ifelse(youtube$average_yearly_earnings.log > 14.14, 1, 0)
table(youtube$average_yearly_earnings.binary)
```

The code above shows the number of each class in the newly created binary target variable when using the mean and median as the splitting threshold respectively. We can see that using the median produces a much better balance between the positive and negative classes this is important as the balance between positive and negative classes in the target variable can significantly impact the performance and interpretation of a machine learning model. When there's a strong class imbalance, the majority class can dominate the minority class, leading to poor predictive performance for the minority class. A model may achieve high overall accuracy simply by predicting the majority class all the time, but it's likely that the recall or precision for the minority class will be very low.

Therefore, going forward we will use the median to split the target variable for the binary classification tasks in the remainder of this notebook.

```{r}
outcome <- "average_yearly_earnings.binary"
pos <- 1

# Remove highly correlated cols
youtube <- subset(youtube, select=-c(average_yearly_earnings.log, highest_yearly_earnings.log, lowest_yearly_earnings.log, highest_monthly_earnings.log, lowest_monthly_earnings.log, video_views_for_the_last_30_days.log, subscribers_for_last_30_days.log, video_views_rank.log))
```


# Splitting the data

We will use a 90/10 split for the data. Given that the data set is relatively small (1000 obvs), we allocate a larger proportion for training to ensure that the model has enough data to learn for. We will also not split the data into train, calibration and test, instead opting to just split the dataset into train and test. In practice, the validation set is used to provide an unbiased evaluation of a model fit on the training data and to tune hyperparameters. However, we can achieve a similar result with k-fold cross validation and have more data to use for training and testing.

```{r}
# This function splits a dataframe into training and test sets given a particular ratio
# @param df the dataframe to operate on
# @param feature the feature to use as the vector of outcomes
# @param train_ratio the split to use
split_data <- function(df, feature, train_ratio = 0.9) {
  train_indices <- createDataPartition(df[,feature],  p= train_ratio, list = FALSE)
  
  # Split the data into training and test sets
  train_set <- df[train_indices, ]
  test_set <- df[-train_indices, ]
  
  # Return a list containing the training and test sets
  list(train = train_set, test = test_set)
}

# Retain only the log transformed variables
youtube_cleaned <- youtube[, grepl("\\.log$", names(youtube))]
youtube_cleaned <- bind_cols(youtube_cleaned, youtube[, c("average_yearly_earnings.binary", "Country", "channel_type", "created_month")])

splits <- split_data(youtube_cleaned, "average_yearly_earnings.binary", train_ratio = 0.9)
train_data <- splits$train
test_data <- splits$test
```

```{r, echo=FALSE}
# The most sketch shit ever
deal_with_zeroes <- function(df){
  if(any(is.na(df))) {
    df[is.na(df)] <- 1e-6  # this is sketch af FIX THIS ASAP
  }
  cols_to_modify <- setdiff(names(df), "average_yearly_earnings.binary")

  df[cols_to_modify] <- lapply(df[cols_to_modify], function(x) ifelse(x == 0, 1e-6, x))
  return(df)
}
youtube_cleaned <- deal_with_zeroes(youtube_cleaned)
train_data <- deal_with_zeroes(train_data)
test_data <- deal_with_zeroes(test_data)
```


Split the data into categorical and numerical variables:

```{r}
# This function seperates a dataframe into categorical and numeric variables
# @param df the dataframe to operate on
separate_vars <- function(df) {
  numeric_var_names <- names(df)[sapply(df, is.numeric)]
  factor_var_names <- names(df)[sapply(df, is.character)]

  numVars <- subset(df, select=numeric_var_names)

  catVars <- subset(df, select=factor_var_names)
  
  return(list(numVars = numVars, catVars = catVars))
}

sep_train <- separate_vars(train_data)
numVars <- sep_train$numVars
catVars <- sep_train$catVars
```


# Single-Variable Models

The following function is used to process the categorical variables and perform single variable prediction for a given categorical column.

```{r}
mkPredC <- function(outCol, varCol, appCol) {
  pPos <- sum(outCol==pos)/length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab/sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}

for (v in colnames(catVars)) {
  pi <- paste('pred', v, sep='')
  train_data[,pi] <- mkPredC(train_data[,outcome], train_data[,v], train_data[,v])
  test_data[,pi] <- mkPredC(test_data[,outcome], test_data[,v], test_data[,v])
}
```

The following function is used to process the numeric variables and perform single variable prediction for a given numeric column.

```{r}
# for numerical variables, we convert them into categorical one and
# call the `mkPredC` function above.
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(as.numeric(
    quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T)))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}

# now go through all the numerical variables in the `numericVars` vector
# and perform the predictions. Again, the outputs are stored back into
# the data frame.
for (v in colnames(numVars)) {
  pi <- paste('pred', v, sep='')
  train_data[,pi] <- mkPredN(train_data[,outcome], train_data[,v], train_data[,v])
  test_data[,pi] <- mkPredN(test_data[,outcome], test_data[,v], test_data[,v])
}
```

## Evaulating Single Variable Models using AUC

The AUC, or Area Under the Receiver Operating Characteristic (ROC) Curve, is a commonly used metric in binary classification to evaluate the performance of a classifier. The AUC value ranges between 0 and 1, and a completely random classifier (a model with no discriminative power) is expected to have an AUC of 0.5.

```{r}
calcAUC <- function(predcol, outcol) {
  perf <- performance(prediction(predcol, outcol==pos),'auc')
  as.numeric(perf@y.values)
}
```

```{r, message=TRUE}
for(v in colnames(catVars)) {
  pii <- paste('pred',v,sep='')
  aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])

  print(sprintf(
    "%s, trainAUC: %4.3f",
    pii, aucTrain))
  
}
```
```{r}
for(v in colnames(numVars)) {
  pii<-paste('pred',v,sep='')
  aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])
  
  if(aucTrain>=0.3) {
    print(sprintf(
      "%s, trainAUC: %4.3f",
      pii,aucTrain))
  }
}
```

```{r, fig.height=4}
ggplot(data=train_data) +
  geom_density(aes(x=predcreated_date.log,color=as.factor(average_yearly_earnings.binary)))
```

The single variable models do not perform well. Each and every variable has and AUC under 0.5. This signifies that using these variables as predictors performs worse than random guessing. In practical terms, if we were to reverse the predictions of such a classifier (i.e., predict class 0 as class 1), it would perform better than its current configuration. 

The double density plot above illustrates why this is the case perfectly. In the context of classifier predicted probabilities, a double density plot is typically used to compare the distributions of predicted probabilities for two classes. In particular, we can see the separation of the classes in this plot.

The double density plot above illustrates the distributions of predicted probabilities for the `predcreated_date.log` feature which reported the highest AUC score in the previous test. We can see that the distributions overlap for the entire plot. Given this significant overlap across the entire range of data, we can see that the classifier has no idea what to predict which explains what it performs worse than the random classifier. 

## Evaulating Single Variable Models using Log likelihood

```{r}
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1-ypred)), na.rm=T)
}
```

Compute the log likelihood of the Null model

```{r}
# Compute the likelihood of the Null model on the train set
logNull <- logLikelihood(sum(train_data[,outcome]==pos)/nrow(train_data), train_data[,outcome]==pos)

cat("The log likelihood of the Null model is:", logNull)
```
The null model, which predicts the same probability of positive outcome for every observation based on the overall proportion of positive outcomes in the training data, has a log likelihood of -592.2145. In general, higher (i.e., closer to 0) log likelihood values indicate better fit between the predicted probabilities and the true outcomes.

Next we run through categorical variables to select top performers:

```{r}
# selCatVars is a vector that keeps the names of the top performing categorical variables.
selCatVars <- c()
minDrop <- 0  # may need to adjust this number

for (v in colnames(catVars)) {
  pi <- paste('pred', v, sep='')
  devDrop <- 2*(logLikelihood(train_data[,pi], train_data[,outcome]==pos) - logNull)
  if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selCatVars <- c(selCatVars, pi)
  }
}
selCatVars
```
Run through numerical variables to select top performers

```{r}
selNumVars <- c()
minDrop <- -10  # may need to adjust this number
for (v in colnames(numVars)) {
  pi <- paste('pred', v, sep='')
  devDrop <- 2*(logLikelihood(train_data[,pi], train_data[,outcome]==pos) - logNull)
  print(devDrop)
  if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selNumVars <- c(selNumVars, pi)
  }
}
selNumVars
```

# Feature Selection

## Forward Selection

Forward selection is a stepwise method used in the context of regression modeling and other predictive modeling techniques. Its main aim is to select a subset of predictors (also known as features or variables) that contribute the most to the prediction of the dependent variable, while discarding predictors that do not add significant predictive power. The process is used to combat overfitting, simplify models, and possibly improve prediction performance.

In this project, we perform forward selection using Akaike Information Criterion (AIC). AIC balances two essential aspects of modeling, the goodness-of-fit and the number of parameters used. Models that fit the data very well but are overly complex (use many parameters) are penalized.

AIC is formulated as:

AIC = 2k - 2ln(L)

Where:

- k is the number of parameters in the model.
- L is the likelihood of the model.

The objective in using AIC for model selection is to find the model that has the lowest AIC value. Lower AIC values suggest better model fit when penalizing for the number of predictors, ensuring that we don't overfit with too many variables.

```{r}
# This function performs feature selection using forward selection
# @param df The dataframe that contains the features
# @param response The response variable
forward_selection <- function(df, response) {
  # Get names of variables minus response variable
  predictors <- setdiff(names(df), response)
  # What features we select
  included <- c()
  best_aic <- Inf
  
  # Loop to iteratively add predictors
  while(length(predictors) > 0) {
    aics <- rep(Inf, length(predictors))
    
    # Test each predictor
    for(j in seq_along(predictors)) {
      formula <- paste(response, "~", paste(c(included, predictors[j]), collapse = "+"))
      model <- lm(formula, data = df)
      aics[j] <- AIC(model)
    }
    
    # If we find a predictor that reduces the AIC, we include it
    if(min(aics) < best_aic) {
      best_aic <- min(aics)
      include <- predictors[which.min(aics)]
      included <- c(included, include)
      predictors <- setdiff(predictors, include)
    } else {
      break
    }
  }
  
  return(included)
}
```


```{r, echo=FALSE}
clean_data <- function(df){
  df1 <- subset(df, select=-c(Country, channel_type, created_month))
  df1 <- df1 %>%
  rename(
    Country = predCountry,
    channel_type = predchannel_type,
    created_month = predcreated_month
  )
  
  cols_to_keep <- !grepl("^pred", names(df))
  df1 <- df1[, cols_to_keep]
  return(df1)
}

train_data <- clean_data(train_data)
test_data <- clean_data(test_data)
youtube_cleaned <- rbind(train_data, test_data)
```

We will create new dataframes for the training, calibration and test sets based on the features from forward selection.

```{r}
selected_features <- forward_selection(train_data, "average_yearly_earnings.binary")

train_forward_sel <- subset(train_data, select=c(selected_features))
test_forward_sel <- subset(test_data, select=c(selected_features))

train_forward_sel$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
test_forward_sel$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)

print(selected_features)
```

## Fisher's Score

Fisher's score, is a method used in feature selection. It assesses the discriminating power of individual features with respect to different classes. It's especially effective for datasets with continuous features and categorical class labels.

The fundamental idea behind Fisher's score is to rank features based on their ability to differentiate between classes. This ability is gauged by considering the ratio of between-class variance to within-class variance.

The function below ranks the features based on their Fisher's Score.

```{r}
fisher_score <- function(train_data, target_column) {
  
  # Extract unique class labels
  classes <- unique(train_data[[target_column]])
  
  if(length(classes) != 2) {
    stop("The provided data does not have two classes. Fisher's Score here is designed for binary classification.")
  }
  
  # Split the data based on classes
  class1_data <- train_data[train_data[[target_column]] == classes[1], ]
  class2_data <- train_data[train_data[[target_column]] == classes[2], ]
  
  # Compute means for each class without target column
  class1_means <- colMeans(class1_data[, !(names(class1_data) %in% target_column)])
  class2_means <- colMeans(class2_data[, !(names(class2_data) %in% target_column)])
  
  # Compute variance for each class without target column
  class1_variances <- apply(class1_data[, !(names(class1_data) %in% target_column)], 2, var)
  class2_variances <- apply(class2_data[, !(names(class2_data) %in% target_column)], 2, var)
  
  # Compute Fisher's Score for each feature
  between_class_variance <- (class1_means - class2_means)^2
  within_class_variance <- class1_variances + class2_variances
  
  fisher_scores <- between_class_variance / within_class_variance
  
  # Sort features by Fisher's Score in descending order and get their names
  ranked_feature_names <- names(sort(fisher_scores, decreasing = TRUE))
  
  return(ranked_feature_names)
}
```

When considering the top 5 features based on their Fisher score, they are the same as those returned through forward selection. It is only at the 6th and 7th features that we see a difference. Using the Fisher score gives us `subscribers.log` and `Population.log` over `Gross.tertiary.education.enrollment.....log` and `created_month` from forward selection. 

```{r}
fisher_features <- fisher_score(train_data, "average_yearly_earnings.binary")
k <- 7
top_fisher_features <- fisher_features[1:k]

train_fisher <- subset(train_data, select=c(top_fisher_features))
test_fisher <- subset(test_data, select=c(top_fisher_features))

train_fisher$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
test_fisher$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)


print(top_fisher_features)
```


## Principal Component Analysis (PCA)

Many machine learning problems face the problem of having too many features which makes the training process very slow and also can possibly make it hard to converge on an optimal solution, which is referred to as the **curse of dimensionality**. One of the approaches that can be used to solve this is through dimensionality reduction techniques. This not only speeds up training, it also makes it significantly easier to visualize the data, reducing a training set to 2 or 3 dimensions can make it possible to plot a condensed view of a high dimensional training set and gain insights on potential clusters in the data. 

Principal Component Analysis (PCA) is one such dimensionality reduction technique. PCA identifies the hyper plane that preserves the maximum variance of the data and then projects the data onto it. The first principal component is the axis that accounts for the largest amount of the variance in the training set, and the second principal component is orthogonal to the first one, that accounts for the largest amount of the remaining variance.

```{r}
# Perform PCA
pca <- prcomp(subset(train_data, select=-c(average_yearly_earnings.binary)), center = TRUE, scale. = TRUE, rank. = 2)

# Get the explained variance ratio
explained_variance_ratio <- pca$sdev^2 / sum(pca$sdev^2)

# Convert principal components and cn_train_y into a data frame for ggplot
df_pca <- as.data.frame(pca$x)
df_pca$class <- as.factor(train_data$average_yearly_earnings.binary)

# Create a scatter plot with different colors for the two classes using ggplot2
ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
  geom_point(aes(shape = class), size = 3) +
  scale_color_manual(values = c("coral1", "cornflowerblue"), name = "Class") +
  labs(x = sprintf('Principal Component 1 (%.2f%%)', explained_variance_ratio[1]*100),
       y = sprintf('Principal Component 2 (%.2f%%)', explained_variance_ratio[2]*100),
       title = 'Scatter Plot of Data on First Two Principal Components') +
  theme_minimal() +
  theme(legend.position = "top")
```

The plot above shows the scatter plot of the data on the first two principal components. We can observe the following:

Class Separation: The scatter plot shows that the two classes are linearly separable in the reduced two-dimensional space. This indicates that the first two principal components capture meaningful information that helps distinguish between the classes.

The explained variance ratio provides insight into the relative importance or contribution of each principal component in capturing the underlying structure of the data. Higher values indicate that the corresponding principal component retains more information about the original dataset.

These values indicate that the first principal component explains approximately 28.49% of the total variance in the dataset, while the second principal component explains approximately 13.78% of the total variance. Together, these two components account for a total of approximately 42.47% of the total variance in the data.

The first principal component captures a significant portion of the variance, indicating that it carries valuable information and potentially separates the classes to a certain extent. The second principal component also contributes to the overall variance, but to a lesser extent compared to the first component.


```{r}
df_pca <- as.data.frame(pca$x)
df_pca$class <- as.factor(train_data$average_yearly_earnings.binary)

variable_vectors <- pca$rotation[, 1:2]

p <- ggplot(df_pca, aes(x = PC1, y = PC2, color = class)) +
  geom_point(aes(shape = class), size = 3, alpha = 0.5) +   # Semi-transparent points
  scale_color_manual(values = c("coral1", "cornflowerblue"), name = "Class") +
  theme_minimal() +
  theme(legend.position = "top") +
  labs(x = 'Principal Component 1',
       y = 'Principal Component 2',
       title = 'Biplot of Data on First Two Principal Components')

variable_vectors_df <- as.data.frame(variable_vectors)
variable_vectors_df$label <- rownames(variable_vectors)

multiplier <- 2

p <- p + 
     geom_segment(data = variable_vectors_df, 
                  aes(x = 0, y = 0, 
                      xend = PC1 * 5 * multiplier, yend = PC2 * 5 * multiplier), 
                  arrow = arrow(type = "closed", length = unit(0.15, "inches")), 
                  color = 'black', alpha = 0.5) + 
     geom_text(data = variable_vectors_df, 
               aes(x = PC1 * 5.2 * multiplier, y = PC2 * 5.2 * multiplier, label = label), 
               vjust = -0.5, hjust = 0, alpha = 0.8, inherit.aes = FALSE)

# Display the plot
print(p)
```

The plot above shows the biplot of the data on the first two principal components. The horizontal axis represents the first principal component (PC1), which captures the most significant variation in the data. The vertical axis represents the second principal component (PC2), which captures the second most significant variation orthogonal to PC1.

- The features are represented as arrows in the plot
- The direction and length of the arrows indicate the contribution of each variable to the principal components.
- Variables pointing in the same direction are positively correlated, while those pointing in opposite directions are negatively correlated.
- Longer arrows represent variables with a stronger influence on the principal components.

**Interpreting the plot**:

- To the left of the plot, there is a cluster of highly correlated features along the axis of the first principal component. There are the demographical features that describe the country of origin of the YouTube channels. 
- On the right of the plot, we have long arrows for `Country`, `channel_type` along the axis of the first principal component. These features also have significant contribution towards this principal component but are negatively correlated with the demographic features.
- Toward the top of the plot, we have long arrows along the axis of the second principal component for `video.views` and `subscribers`. We also have a smaller arrow for `uploads`. This suggests that the second principal components is highly influenced by these features.
- `created_year`, `created_month` and `created_date` have very small arrows, and are practically non-existent. This mean that they have little influence on either of the first 2 principal components. 

### Using PCA to retain 95% of the explained variance

```{r, fig.height=4}
# Compute the PCA without limiting to 2 PC
pca <- prcomp(train_data, center = TRUE, scale. = TRUE)
explained_variance_ratio <- pca$sdev^2 / sum(pca$sdev^2)
cumulative_variance <- cumsum(explained_variance_ratio)
num_components <- which(cumulative_variance >= 0.95)[1]

explained_variance_data <- data.frame(
  Dimension = 1:length(explained_variance_ratio),
  ExplainedVariance = explained_variance_ratio,
  CumulativeVariance = cumulative_variance
)


ggplot(explained_variance_data, aes(x = Dimension)) +
  geom_bar(aes(y = ExplainedVariance), stat = "identity", fill = "blue", alpha = 0.7) +
  geom_line(aes(y = CumulativeVariance), color = "red", size = 1.2) +
  geom_point(aes(y = CumulativeVariance), color = "red") +
  geom_vline(aes(xintercept = 10), color = "black", linetype = "dashed", size = 1) +
  annotate("text", x = num_components, y = 0.8, label = "Number of components required\nto retain 95% of the\nexplained variance", hjust = 0.25) +
  labs(
    title = "Explained Variance as a Function of Dimensions",
    x = "Number of Dimensions",
    y = "Variance Explained",
    caption = "Blue Bars = Individual Explained Variance\nRed Line = Cumulative Explained Variance"
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()

```


The explained variance ratio indicates the proportion of the datasetâ€™s variance that lies along each principal component. As seen in the first plot, the first 2 principal components capture only 57.45% of the variance in the dataset. This is not sufficient information as we are losing more than 40% of the explained variance. In order for a more accurate prediction, we would want to retain at least 95% of the explained variance. 

The plot above shows the explained variance as a function of number of dimensions. The line in red shows the cumulative explained variance and the bars in blue shows the individual explained variance of each principal component. It can be observed that after 10 dimensions, we have captured 95% of the explained variance in the data and adding any subsequent dimensions provides very little for the increasing complexity. 

```{r}
# Function to apply PCA transformation on new datasets
transform_data <- function(data, pca, num_components) {
  centered_data <- scale(data, center = pca$center, scale = pca$scale)
  pc_scores <- centered_data %*% pca$rotation[, 1:num_components]
  return(as.data.frame(pc_scores))
}


train_pca <- as.data.frame(pca$x[, 1:num_components])
test_pca <- transform_data(test_data, pca, num_components)

train_pca$average_yearly_earnings.binary <- as.factor(train_data$average_yearly_earnings.binary)
test_pca$average_yearly_earnings.binary <- as.factor(test_data$average_yearly_earnings.binary)
```


# Evaluation of Models

## Functions 

We will evaluate the Decision Tree and Logistic Regression Models using the 4 functions shown below:

This function calculates the Accuracy, Precision, Recall and F1 score of a given model.

- **Accuracy**: Measures the fraction of correct predictions made by the model out of all predictions. 

  - Accuracy = Number of correct predictions / total number of all predictions 
  
- **Precision**: Measures the fraction of relevant instances among the retrieved instances.Out of all the positive predictions made by the model, how many were actually positive.

  - Precision = True Positive (TP) / (True Positive + False Positive)
  
  - Precision is especially important when the cost of a false positive is high.
  
- **Recall**: Measures the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Out of all the actual positives, how many were predicted as positive by the model.

  - Recall = True Positive (TP) / (True Positive + False Negatives)
  
  - Recall is crucial when the cost of a false negative is high. 
  
- **F1 Score**: Harmonic mean of precision and recall. The F1 score demonstrates the precision/recall trade off. The F1 score cannot be high unless both precision and recall are high.

  - The F1 score ranges between 0 (worst) and 1 (best). An F1 score of 1 indicates perfect precision and recall, while a score of 0 indicates that either the precision or the recall is zero.

```{r}
# This function calculates the accuracy, precision, recall and F1 score for a model
cal_scores <- function(train_data, train_pred, calibration_data, calibration_pred){
  
  # Calculate metrics for the training data
  train_results <- tibble(
    Accuracy  = accuracy_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
    Precision = precision_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
    Recall    = recall_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class),
    F1        = f_meas_vec(train_data$average_yearly_earnings.binary, train_pred$.pred_class)
  )
  
  # Calculate metrics for the calibration data
  calibration_results <- tibble(
    Accuracy  = accuracy_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
    Precision = precision_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
    Recall    = recall_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class),
    F1        = f_meas_vec(calibration_data$average_yearly_earnings.binary, calibration_pred$.pred_class)
  )
  
  list(train = train_results, calibration = calibration_results)
}
```

This function visualizes the precision and recall as a function of the threshold. By plotting precision and recall against the threshold, one can visually inspect the trade-offs. At some thresholds, precision might be high but recall is low and vice versa. Understanding this trade-off can help in deciding the optimal threshold.

```{r}
# This function plots precision and recall vs threshold
plot_precision_recall_vs_threshold <- function(pred, data, title) {
  # Predict the probabilities
  probs <-pred$.pred_1
  
  # Create a data frame with true labels and predicted probabilities
  results <- data.frame(
    True = data$average_yearly_earnings.binary,
    Prob = probs
  )
  
  # For each threshold, compute precision and recall
  thresholds <- seq(0, 1, by = 0.01)
  metrics <- sapply(thresholds, function(thresh) {
    predictions <- ifelse(results$Prob > thresh, 1, 0)
    # Ensure predictions always have levels 0 and 1
    predictions <- factor(predictions, levels = c(0, 1))
    
    precision <- precision_vec(results$True, predictions)
    recall <- recall_vec(results$True, predictions)
    c(Precision = precision, Recall = recall)
  })
  
  # Transform results for plotting
  df <- as.data.frame(t(metrics))
  df$Threshold <- thresholds
  df$Difference <- abs(df$Precision - df$Recall)
  intersection_point <- df[which.min(df$Difference), "Threshold"]
  
  p <- ggplot(df, aes(x = Threshold)) +
    geom_line(aes(y = Precision, color = 'Precision')) +
    geom_line(aes(y = Recall, color = 'Recall')) +
    labs(title = title, 
         y = "Value", 
         x = "Threshold",
         color = "Metric") +
    geom_vline(aes(xintercept = 0.5), linetype = "dashed", color = "black", size = 0.5) +
    geom_text(aes(x = 0.5, y = 0.2, label = "Default threshold"), angle = 90, vjust = -0.5) +
    geom_vline(aes(xintercept = intersection_point), linetype = "dashed", color = "red", size = 0.5) +
    geom_text(aes(x = intersection_point, y = 0.2, 
                  label = paste0("Intersection ", round(intersection_point, 2))), angle = 90, vjust = -0.5) +
    theme_minimal()
  
  return(p)
}
```

This function plots the Receiver Operating Characteristic (ROC) and displays the Area Under Curve (AUC). The Receiver Operating Characteristic (ROC) is a graphical representation used to evaluate the performance of binary classification models. It illustrates the diagnostic ability of a classifier system as its discrimination threshold changes. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) for various threshold values.

AUC is a metric that quantifies the overall ability of the classifier to distinguish between positive and negative classes. An AUC of 1 indicates a perfect classifier, while an AUC of 0.5 indicates no discrimination capability. AUC values between 0 and 0.5 suggest a model that performs worse than random guessing.

```{r}
# This function plots the ROC 
plot_roc <- function(predictions, data, title) {
  # Predict the probabilities
  probs <- predictions$.pred_1
  pred <- prediction(probs, data$average_yearly_earnings.binary)
  perf <- performance(pred, "tpr", "fpr")
  roc_data <- data.frame(
    FPR = unlist(perf@x.values),
    TPR = unlist(perf@y.values)
  )
  auc <- performance(pred, measure = "auc")
  auc_value <- unlist(auc@y.values) 
  ggplot(roc_data, aes(x = FPR, y = TPR)) +
    geom_line(color = "blue") +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    geom_text(aes(x = 0.5, y = 0.25), 
              label = paste("AUC =", round(auc_value, 2)), 
              color = "red") +
    labs(title = title,
         x = "False Positive Rate",
         y = "True Positive Rate") +
    coord_fixed(ratio = 1) +
    theme_minimal()
}
```

This function plots the confusion matrix. The confusion matrix, is a table used to evaluate the performance of a classification model. It provides a breakdown of predictions into a table showing correct predictions and the types of incorrect predictions made. It is particularly useful for a more detailed analysis of where a classification model is getting things right and where it's making errors.

```{r}
# Plot confusion matrix
plot_con_matrix <- function(pred, data, title){
  cm <- confusionMatrix(as.factor(pred[[1]]), data$average_yearly_earnings.binary)
  plt <- as.data.frame(cm$table)
  plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
  ggplot(plt, aes(Prediction,Reference, fill= Freq)) +
          geom_tile() + geom_text(aes(label=Freq)) +
          scale_fill_gradient(low="white", high="#009194") +
          labs(title = title, x = "Reference",y = "Prediction") 
}
```

# Logistic Regression Classifier

The first classification method used will be logistic regression.  Logistic Regression is a statistical method used for modeling the probability of a certain class (high/low earnings). It's especially used in cases where the response variable is categorical, most commonly binary. We have binarized our target variable in an earlier part of this report to make it compatible with the logistic regression classifier. 

As mentioned earlier in the report when we were splitting the data into training and testing sets, we did not make use of a calibration set as we were going to use k-fold cross-validation to pick the best model. K-fold cross validation is a widely used technique for assessing the performance of machine learning models. With a single train/test split, the evaluation can vary a lot based on which data points end up in the training set and which ones end up in the test set. K-fold cross validation, by averaging results from multiple partitions, reduces this variability and provides a more stable and robust estimate of model performance.

The function below allows us to train a logistic regression model given a dataset. 

```{r}
# Train a logistic regression model using glm and 10 fold cross validation
train_log_reg <- function(df) {
  trControl <- trainControl(method = "cv", number = 10)
  model <- train(
    average_yearly_earnings.binary ~ ., 
    data = df, 
    method = "glm", 
    family = "binomial", 
    trControl = trControl
  )
  return(model)
}
```

## Logistic Regression with Forward Selection Features

```{r}
log_reg_forward <- train_log_reg(train_forward_sel)
print(log_reg_forward)


train_pred_logreg <- as_tibble(predict(log_reg_forward, newdata = train_forward_sel, type = "prob")) %>%
  mutate(".pred_1" = `1`)

test_pred_logreg <- as_tibble(predict(log_reg_forward, newdata = test_forward_sel, type = "prob")) %>%
  mutate(".pred_1" = `1`)

train_class_logreg <- as_tibble(ifelse(train_pred_logreg$.pred_1 > 0.5, 1, 0)) %>%
  mutate(".pred_class" = as.factor(value))

test_class_logreg <- as_tibble(ifelse(test_pred_logreg$.pred_1 > 0.5, 1, 0)) %>%
  mutate(".pred_class" = as.factor(value))
```

We train a logistic regression model based on the features obtained from forward selection. The metrics of the best performing model are:

- Accuracy: This is the proportion of correctly predicted classifications in the dataset. An accuracy of 0.7999179 (or roughly 80%) means that 80% of the predictions made by the model on the test sets were correct.

- Kappa: Cohen's Kappa is a statistic that measures the agreement between two raters (in this case, the model's predictions and the true values) while taking into account the agreement that could happen by chance. A value of 0.6108251 indicates a moderate to substantial agreement. Typically, a Kappa value above 0.6 is considered substantial.

```{r}
importances <- varImp(log_reg_forward, scale = TRUE)
print(importances)
```


```{r}
performance_log_reg_forward_sel <- cal_scores(train_forward_sel, train_class_logreg , test_forward_sel, test_class_logreg)
print(performance_log_reg_forward_sel$train)
print(performance_log_reg_forward_sel$calibration)
```

The logistic regression classifier trained on the forward selection features performed well on both the training and test set. It also had a good balance between precision and recall as shown by the F1 score of 0.83. The classifier performs slightly better on the test set than the training set in terms of all four metrics (Accuracy, Precision, Recall, and F1 Score). This is a positive indication as it suggests that the model is generalizing well and is not overfitting to the training data.

```{r, fig.width=10, fig.height=4}
p1 <- plot_precision_recall_vs_threshold(train_pred_logreg, train_forward_sel, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_logreg, test_forward_sel, "Precision and Recall vs. Threshold, Test Set")
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_roc(train_pred_logreg, train_forward_sel, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_logreg, test_forward_sel, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
```

The AUC, which stands for Area Under the Receiver Operating Characteristic Curve, is a metric that provides an aggregate measure of the classifier's performance across all possible classification thresholds. The AUC values are robust both on the training(0.89) and test sets(0.93), indicating that the logistic regression classifier has strong discriminative power.

The fact that the test AUC (0.93) is higher than the train AUC (0.89) suggests that the model is generalizing even better to unseen data, at least in terms of rank-ordering instances by predicted probability. 

The choppy nature of the ROC curve for the test set could be due to the fact that the test set only contains 94 observations compare to the 855 instances in the training set. The ROC curve may appear to be more jagged or choppy simply due to the granularity of the data points. Each data point can result in a significant jump in the curve.

```{r, fig.width=8, fig.height=4}
p1 <- plot_con_matrix(train_class_logreg, train_forward_sel, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_logreg, test_forward_sel, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
```

## Logistic Regression Classifier with Fisher Score Features

```{r}
log_reg_fisher <- train_log_reg(train_fisher)
print(log_reg_fisher)

train_pred_fisher <- as_tibble(predict(log_reg_fisher, train_fisher, type = "prob")) %>%
  mutate(".pred_1" = `1`)

test_pred_fisher <- as_tibble(predict(log_reg_fisher, test_fisher, type = "prob")) %>%
  mutate(".pred_1" = `1`)

train_class_fisher <- as_tibble(ifelse(train_pred_fisher$.pred_1 > 0.5, 1, 0)) %>%
  mutate(".pred_class" = as.factor(value))

test_class_fisher <- as_tibble(ifelse(test_pred_fisher$.pred_1 > 0.5, 1, 0)) %>%
  mutate(".pred_class" = as.factor(value))
```

We train a logistic regression model based on the features obtained from Fisher's Score. The metrics of the best performing model are:

- Accuracy: This is the proportion of correctly predicted classifications in the dataset. An accuracy of 0.7987962 (or roughly 80%) means that 80% of the predictions made by the model on the test sets were correct.

- Kappa: Cohen's Kappa is a statistic that measures the agreement between two raters (in this case, the model's predictions and the true values) while taking into account the agreement that could happen by chance. A value of 0.5971552 indicates a moderate to substantial agreement. Typically, a Kappa value above 0.6 is considered substantial.

```{r}
importances <- varImp(log_reg_fisher, scale = TRUE)
print(importances)
```


```{r}
performance_log_reg_fisher <- cal_scores(train_fisher, train_class_fisher, test_fisher, test_class_fisher)
print(performance_log_reg_fisher$train)
print(performance_log_reg_fisher$calibration)
```

The logistic regression classifier trained on the Fisher's Score features performed well on both the training and test set. However, the model on the test set outperformed the model on the training set significantly, the most out of any of the classifiers tested in this report. This is a positive indication as it suggests that the model is generalizing well and is not overfitting to the training data.  

```{r, fig.width=10, fig.height=4}
p1 <- plot_precision_recall_vs_threshold(train_pred_fisher, train_fisher, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_fisher , test_fisher, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_roc(train_pred_fisher, train_fisher, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_fisher, test_fisher, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
```

The ROC plot once again demonstrates that this model performs significantly better on the testing set when compared to the training set.

```{r, fig.width=8, fig.height=4}
p1 <- plot_con_matrix(train_class_fisher, train_fisher, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_fisher, test_fisher, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
```

## Logistic Regression Classifier with PCA Features

```{r}
log_reg_pca <- train_log_reg(train_pca)
print(log_reg_pca)

train_pred_pca <- as_tibble(predict(log_reg_pca, train_pca, type = "prob")) %>%
  mutate(".pred_1" = `1`)

test_pred_pca <- as_tibble(predict(log_reg_pca, test_pca, type = "prob")) %>%
  mutate(".pred_1" = `1`)

train_class_pca <- as_tibble(ifelse(train_pred_pca$.pred_1 > 0.5, 1, 0)) %>%
  mutate(".pred_class" = as.factor(value))

test_class_pca <- as_tibble(ifelse(test_pred_pca$.pred_1 > 0.5, 1, 0)) %>%
  mutate(".pred_class" = as.factor(value))
```

We train a logistic regression model based on the features obtained from PCA with 10 princial components. The metrics of the best performing model are:

- Accuracy: This is the proportion of correctly predicted classifications in the dataset. An accuracy of 0.9988235 means that 99% of the predictions made by the model on the test sets were correct.

- Kappa: Cohen's Kappa is a statistic that measures the agreement between two raters (in this case, the model's predictions and the true values) while taking into account the agreement that could happen by chance. A value of 0.9976461 indicates a extreme agreement. Typically, a Kappa value above 0.6 is considered substantial.

```{r}
importances <- varImp(log_reg_pca, scale = TRUE)
print(importances)
```

```{r}
performance_log_reg_pca <- cal_scores(train_pca, train_class_pca, test_pca, test_class_pca)
print(performance_log_reg_pca$train)
print(performance_log_reg_pca$calibration)
```

The logistic regression classifier using PCA achieved a perfect score on all metrics for the training set. This means that every single instance in your training and testing dataset was correctly classified, with no false positives or false negatives. Achieving perfect scores across both training and test sets is extremely rare, especially for real-world, complex datasets. 

```{r, fig.width=10, fig.height=4}
p1 <- plot_precision_recall_vs_threshold(train_pred_pca, train_pca, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_pca , test_pca, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
```

The plot above illustrates the tradeoff between precision and recall versus threshold. As seen previously, both precision and recall are 1 so there is no trade off until we get to the extremes of the threshold being 0 or 1.

```{r, fig.width=8, fig.height=4}
p1 <- plot_roc(train_pred_pca, train_pca, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_pca, test_pca, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
```

An AUC of 1 indicates that there is a 100% probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. This suggests that the classifier's decision boundary has perfectly distinguished between the two classes in both the training and test datasets.

```{r, fig.width=8, fig.height=4}
p1 <- plot_con_matrix(train_class_pca, train_pca, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_pca, test_pca, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
```

# Decision Tree

Decision trees are another model type that makes predictions that are piecewise constant. 

The following are the contents in the decision tree that will be discussed:

- Internal Nodes: These nodes represent a decision based on a feature value.

- Decision Rule: Each internal node displays a decision criterion based on a feature and a threshold value. This criterion is employed to divide the dataset into subsets based on the specified feature and threshold.

- Gini Impurity: A node is pure (gini=0) if all training instances it applies to belong to the same class.

- Leaf Nodes: A node that does not have any child nodes (terminal node). This is what the decision tree predicts your class as.

- Samples: The quantity of samples that arrive at each node, including both internal and leaf nodes.

- Class: The most common class label in each node is shown. For leaf nodes, this is used to make predictions when a sample reaches that node.

```{r}
train_dt <- function(df) {
  trControl <- trainControl(method = "cv", number = 10)
  dt_forward_cv <- train(
    average_yearly_earnings.binary ~ ., 
    data = df, 
    method = "rpart", 
    trControl = trControl
  )
  return(dt_forward_cv)
}
```

## Decision Tree Classifier with Forward Selection Features

```{r, fig.height=4, fig.width=4}
dt_forward <- train_dt(train_forward_sel)
print(dt_forward)

train_class_dt <- as_tibble(predict(dt_forward, newdata = train_forward_sel, type = "raw")) %>%
  rename(".pred_class" = value)
test_class_dt <- as_tibble(predict(dt_forward, newdata = test_forward_sel, type = "raw")) %>%
  rename(".pred_class" = value)

train_pred_dt <- as_tibble(predict(dt_forward, newdata = train_forward_sel, type = "prob")) %>%
  rename(".pred_0" = `0`, ".pred_1" = `1`)
test_pred_dt <- as_tibble(predict(dt_forward, newdata = test_forward_sel, type = "prob")) %>%
  rename(".pred_0" = `0`, ".pred_1" = `1`)

rpart.plot(dt_forward$finalModel)
```

- The tree has a depth of 2 which is not usually a sign of overfitting. However, the tree was seen to be very imbalanced, with the majority of splits occurring on the right side.

- A close examination of the leaf nodes of the decision tree reveals that the majority of the leaf nodes in those levels only have a few samples. This may indicate that the model is capturing noise or random fluctuations in the data, rather than meaningful patterns or relationships.

- The tree contains a large number of sparse nodes. Examining the tree structure, it is revealed that a significant number of the samples travel down the extreme left and right of the tree. This leaves the nodes in the center, of which are the majority, to be sparse. These nodes contain very few samples. This may indicate that the model is overfitting the data, capturing noise or random fluctuations in the data.

```{r}
importances <- varImp(dt_forward, scale = TRUE)
print(importances)
```

```{r}
performance_dt_forward <- cal_scores(train_forward_sel, train_class_dt, test_forward_sel, test_class_dt)
print(performance_dt_forward$train)
print(performance_dt_forward$calibration)
```

In terms of accuracy, the model achieved an accuracy of around 82.81% on the training set and 82.98% on the test set, which suggests that it's performing similarly on both sets.

Precision is the ratio of true positive predictions to the total number of positive predictions made by the model. In the context of a decision tree classifier, it means that when the model predicts a positive outcome, it is correct approximately 82.45% of the time on the training set and 79.59% of the time on the test set.

Recall (also known as sensitivity or true positive rate) is the ratio of true positive predictions to the total number of actual positive instances in the dataset. The model's recall is approximately 82.25% on the training set and 86.67% on the test set. This indicates that the model is better at capturing positive instances on the test set compared to the training set.

The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall. The model achieved an F1 score of approximately 82.35% on the training set and 82.98% on the test set.

```{r, fig.width=10, fig.height=4}
p1 <- plot_precision_recall_vs_threshold(train_pred_dt, train_forward_sel, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_dt, test_forward_sel, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
```

The plot above shows the trade-off bewteen precision and recall vs threshold. 

```{r, fig.width=8, fig.height=4}
p1 <- plot_roc(train_pred_dt, train_forward_sel, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_dt, test_forward_sel, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
```

An AUC-ROC value of 0.85 on the training set and 0.86 on the test set indicates that the decision tree classifier is doing a good job of separating the two classes, both in the training and test data. 

A difference of only 0.01 between the training and test set AUC-ROC values suggests that the model is not significantly overfitting or underfitting the data, which is a positive sign. It means that the model's ability to discriminate between classes is consistent between the training and test sets.

```{r, fig.width=8, fig.height=4}
p1 <- plot_con_matrix(train_class_dt, train_forward_sel, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_dt, test_forward_sel, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
```

## Decision Tree Classifier with Fisher Score Features

```{r, fig.height=4, fig.width=4}
dt_fisher <- train_dt(train_fisher)
print(dt_fisher)

train_class_dt <- as_tibble(predict(dt_fisher, newdata = train_fisher, type = "raw")) %>%
  rename(".pred_class" = value)
test_class_dt <- as_tibble(predict(dt_fisher, newdata = test_fisher, type = "raw")) %>%
  rename(".pred_class" = value)

train_pred_dt <- as_tibble(predict(dt_fisher, newdata = train_fisher, type = "prob")) %>%
  rename(".pred_0" = `0`, ".pred_1" = `1`)
test_pred_dt <- as_tibble(predict(dt_fisher, newdata = test_fisher, type = "prob")) %>%
  rename(".pred_0" = `0`, ".pred_1" = `1`)

rpart.plot(dt_fisher$finalModel)
```

The decision tree with Fisher Score features performed identically to the decision tree using forward selection features. This is because the tree made the splits using the same 2 features `video.views` and `uploads` as the other decision tree. 

This indicates that the decision tree classifier ranks the importance of these 2 features very highly. However, this unfortunately did not produce different results for the tree.  

```{r}
importances <- varImp(dt_fisher, scale = TRUE)
print(importances)
```

```{r}
performance_dt_forward <- cal_scores(train_pca, train_class_dt, test_pca, test_class_dt)
print(performance_dt_forward$train)
print(performance_dt_forward$calibration)
```

```{r, fig.width=10, fig.height=4}
p1 <- plot_precision_recall_vs_threshold(train_pred_dt, train_pca, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_dt , test_pca, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_roc(train_pred_dt, train_fisher, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_dt, test_fisher, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_con_matrix(train_class_dt, train_pca, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_dt, test_pca, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
```

## Decision Tree Classifier with PCA Features

```{r, fig.height=4, fig.width=4}
dt_pca <- train_dt(train_pca)
print(dt_pca)

train_class_dt <- as_tibble(predict(dt_pca, newdata = train_pca, type = "raw")) %>%
  rename(".pred_class" = value)
test_class_dt <- as_tibble(predict(dt_pca, newdata = test_pca, type = "raw")) %>%
  rename(".pred_class" = value)

train_pred_dt <- as_tibble(predict(dt_pca, newdata = train_pca, type = "prob")) %>%
  rename(".pred_0" = `0`, ".pred_1" = `1`)
test_pred_dt <- as_tibble(predict(dt_pca, newdata = test_pca, type = "prob")) %>%
  rename(".pred_0" = `0`, ".pred_1" = `1`)

rpart.plot(dt_pca$finalModel)
```

- The tree has a depth of 3 which is not usually a sign of overfitting. However, the tree was seen to be very imbalanced, with the majority of splits occuring on the left side.

- A close examination of the leaf nodes of the decision tree reveals that the majority of the leaf nodes in those levels only have a few samples. This may indicate that the model is capturing noise or random fluctuations in the data, rather than meaningful patterns or relationships.

- The tree contains a large number of sparse nodes. Examining the tree structure, it is revealed that a significant number of the samples travel down the extreme left and right of the tree. This leaves the nodes in the centre, of which are the majority, to be sparse. These nodes contain very few samples. This may indicate that the model is overfitting the data, capturing noise or random fluctuations in the data.

- The decision tree is only making splits based on 2 of the principal components, PC1 and PC2. Decision trees typically split on features that are more important for making accurate predictions. It's possible that the two features the tree is splitting on are the most informative features in your dataset, and the other seven features may not provide much additional information. This is further supported by the earlier plot of principal components during the feature selection stage, where PC1 and PC2 took up a significant portion of the explained variance in the dataset, with the subsequent 7 PCs being worth much less. 

```{r}
importances <- varImp(dt_pca, scale = TRUE)
print(importances)
```

```{r}
performance_dt_forward <- cal_scores(train_pca, train_class_dt, test_pca, test_class_dt)
print(performance_dt_forward$train)
print(performance_dt_forward$calibration)
```

Precision is the ratio of true positive predictions to the total number of positive predictions made by the model. It measures the ability of the model to avoid false positives. A high precision score indicates that when the model predicts a positive class, it is usually correct. In the results, both the training and test set precision values are high, suggesting that the model is good at making positive predictions.

Recall, also known as sensitivity or true positive rate, is the ratio of true positive predictions to the total number of actual positive instances in the dataset. It measures the model's ability to correctly identify all positive instances. The recall values for both the training and test sets are also reasonably high, indicating that the model is effective at capturing most of the positive instances.

The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is especially useful when dealing with imbalanced datasets. A high F1 score indicates that the model has a good balance between making accurate positive predictions (high precision) and capturing all positive instances (high recall). The F1 scores for both the training and test sets are high, which suggests that the model performs well in terms of both precision and recall.

```{r, fig.width=10, fig.height=4}
p1 <- plot_precision_recall_vs_threshold(train_pred_dt, train_pca, "Precision and Recall vs. Threshold, Train Set")
p2 <- plot_precision_recall_vs_threshold(test_pred_dt , test_pca, "Precision and Recall vs. Threshold, Cal Set")
grid.arrange(p1, p2, ncol=2)
```

```{r, fig.width=8, fig.height=4}
p1 <- plot_roc(train_pred_dt, train_pca, "ROC Curve Train Set")
p2 <- plot_roc(test_pred_dt, test_pca, "ROC Curve Test Set")
grid.arrange(p1, p2, ncol=2)
```

These AUC values are also quite high, which is a positive sign. AUC values close to 1 indicate that the model has a strong ability to discriminate between the two classes, which suggests that the decision tree classifier is effective in distinguishing between positive and negative instances.

```{r, fig.width=8, fig.height=4}
p1 <- plot_con_matrix(train_class_dt, train_pca, "Confusion Matrix Train Set")
p2 <- plot_con_matrix(test_class_dt, test_pca, "Confusion Matrix Test Set")
grid.arrange(p1, p2, ncol=2)
```

## Classification Summary

### Model Performance

The best performing model is the logistic regression model with PCA features which scored a perfect score in accuracy, precision and recall. This is not normal behavior for classifiers as in the real world, we rarely see classifiers achieve perfect scores. However, this could be explained by looking at the biplot of the first 2 principal components that was plotted in the feature selection, we observed that just in the first 2 principal components, the data was linearly separable. As we move to higher dimensional spaces by adding other principal components, it might have given use even better separation of the classes which led to the unlikely result of a perfect classifier. 

This is also supported by the fact that on average, the logistic regression classifiers outperformed the decision tree classifiers. Therefore, for the purposes of this classification task, logistic regression seems to be the most suitable.

### Feature Importances

Both logistic regression and decision trees are inteprable classifiers. 

Logistic regression, for binary classification, predicts the probability that a given instance belongs to a particular category based on a linear combination of its features. The model assigns a weight to each feature, and the sign and magnitude of this weight provide information about how the feature affects the outcome.

The decision tree structure is intuitive, with decisions being made at each node based on a single feature. This mirrors human decision-making in some contexts where we use a series of "if-then-else" conditions. As seen earlier, we can visualize this tree to have a clear overview of the decision making process in the model. 

In terms of the feature importance obtained from the two classifiers, the logistic regression classifiers and decision trees prioritize similar features. 

- Logistic Regression: `Country`, `video_views_rank` and `channel_type` were the most important features with `video.views` and `subscribers`receiving very little weight in all the models trained.
- Decision Tree: `video_views` and `uploads` were by far the most impact feature, with it being use to make multiple splits in the same tree. This is completely opposite to the logistic regression models which gave very little weight to these 2 features.   

# Clustering

## Data preparation

We will be using the cleaned dataset and removing the target variable for clustering. We will also scale the data to have a mean value of 0 and a standard deviation of 1. This makes use of the `transform_cols()` function that was defined at the beginning of this document. 

```{r, echo=FALSE}
# sketch
colnames(youtube_cleaned) <- sub("\\.log$", "", colnames(youtube_cleaned))
```


```{r}
youtube_clustering <- subset(youtube_cleaned, select=-c(average_yearly_earnings.binary))
youtube_clustering <- transform_cols(youtube_clustering, colnames(youtube_clustering), "norm")
```

```{r, echo=FALSE}
# sketch
youtube_clustering <- youtube_clustering[, grep("\\.norm$", colnames(youtube_clustering))]
colnames(youtube_clustering) <- sub("\\.norm$", "", colnames(youtube_clustering))
```

## Hierarchical Clustering 

We will first compute distance between all pairs of points in the data using the euclidean distance.

```{r}
plot_dendrogram <- function(distMetric, linkageMethod, k){
  d <- dist(youtube_clustering, method=distMetric)
  pfit <- hclust(d, method=linkageMethod) 
  plot(pfit, main="Cluster Dendrogram for Youtube Channels", labels=FALSE)
  rect.hclust(pfit, k=k) 
  
  return(pfit)
}
```

We will now explore several different linkage criteria for plotting dendrograms.

- Each leaf node represents a Youtuber, in the diagram , they are labelled but it is very difficult to interpret due to the sheer density of the data points. 

- Instances at the bottom of the dendrogram are more similar than those higher up

- For any two instances, we can measure the height of the fusion on the vertical axis to see how different the two instances are. The greater the height, the greater the difference.

### Hierarchical Clustering using single linkage

```{r, fig.height=15, fig.width=20, message=FALSE}
pfit <- plot_dendrogram("manhattan", "single", 3)
```

In single linkage, we treat each data point as a single cluster. Therefore, if there are N data points, we start with N clusters. In each of the subsequent stages, the two clusters that are closest to each other are merged into a single cluster, reducing the total number of clusters by one.The distance between two clusters is determined by the shortest distance between points in the two clusters.

One of the main features of single linkage clustering is its ability to detect elongated or non-elliptical shaped clusters. However, it also has a tendency to create "chained" clusters where clusters can be long and "stringy", potentially linking together clusters that are quite far apart in the broader dataset. This is because only the minimum pairwise distance matters and can lead to the "chaining effect".

This can be observed in the plot of the dendrogram above. `Daddy Yankee` is very separated from the remainder of the instances in the dataset and only merges at a high distance value. For the rest of the observations, the dendrogram displays the chaining effect typical of single linkage, where clusters can be long and "stringy." This effect is especially noticeable in the right cluster, where many subclusters merge at a low height, forming a long chain.

### Hierarchical Clustering using complete linkage

```{r, fig.height=15, fig.width=20, message=FALSE}
pfit <- plot_dendrogram("manhattan", "complete", 3)
```

Complete linkage considers the distance between two clusters to be equal to the greatest distance from any member of one cluster to any member of the other cluster. The distance between two clusters is equal to the farthest points between them.

Compared to other linkage methods such as single linkage or average linkage, complete linkage tends to produce more balanced and compact clusters. However, it can also be sensitive to outliers, as a single outlying point can influence the distance between two clusters.

We can observe this in the dendrogram above, where we once again see the outlier `Daddy Yankee` at the top of the dendrogram. However, unlike single linkage, complete linkage produced a much more balanced dendrogram with no stringy trees. 

### Hierarchical Clustering using ward.D2

Clustering using `ward.D2`:

```{r, fig.height=15, fig.width=20, message=FALSE}
pfit <- plot_dendrogram("manhattan", "ward.D2", 3)

```

The main goal of ward.D2 is to minimize the increase in total within-cluster variance after merging two clusters. In other words, it finds the pair of clusters to merge such that the sum of squared distances between the items and the centroids of their respective clusters increases the least.

Ward's method often results in clusters that are relatively compact and roughly equal in size. However, it assumes that clusters have a roughly spherical shape in the data space. Like other methods, it can sometimes be sensitive to outliers.

Ward's method produced the most balanced tree with 3 distinct, well separated clusters. However, the clusters are not equal in size. As this produced the most balanced dendrogram, we will be using the ward.D2 clustering method for the remainder of this document. 


## Extracting members using cutree()

cutree() takes in a clustering model (e.g., created by hclust()) and returns a vector (or matrix) of cluster group assignment(s) for each row

```{r}
plot_instances <- function(df, x_col, group_vec) {
  
  # Check if x_col exists in df
  if (!x_col %in% colnames(df)) {
    stop("The specified x_col does not exist in the dataframe")
  }
  df$Group <- group_vec
  
  agg_data <- df %>%
    group_by(!!sym(x_col), Group) %>%
    summarise(Count = n()) %>%
    ungroup()
  
  agg_data <- agg_data %>%
    arrange(Count) %>%
    mutate(!!sym(x_col) := factor(!!sym(x_col), levels = unique(!!sym(x_col))))
  
  # Plotting
  p <- ggplot(agg_data, aes_string(y = x_col, x = "Count", fill = "as.factor(Group)")) +
    geom_bar(stat = "identity", position = "stack") +
    labs(title = paste0("Number of Instances by ", x_col, " and Group"),
         x = "Number of Instances",
         y = x_col,
         fill = "Group") +
    theme_minimal() +
    theme(axis.text.y = element_text(hjust = 1))
  
  return(p)
}
```

```{r}
groups <- cutree(pfit, k=3)
plot_instances(names, "Country", groups)
```

```{r}
plot_instances(names, "channel_type", groups)
```

We are unable to extract much useful information from the clusters when plotting by `Country` or `Category`. As seen in the plots above, each variable is made of a mix of the three clusters. It is hard to glean any useful information from them. 

## Visualising Clusters

```{r}
groups <- cutree(pfit, k=2)
princ <- prcomp(youtube_clustering)
nComp <- 2
project2D <- as.data.frame(predict(princ, newdata=youtube_clustering)[,1:nComp])
hclust.project2D <- cbind(project2D, cluster=as.factor(groups), country=names$Country)
head(hclust.project2D)

```

```{r}
library('grDevices')
find_convex_hull <- function(proj2Ddf, groups) {
  do.call(rbind,
    lapply(unique(groups),
      FUN = function(c) {
        f <- subset(proj2Ddf, cluster==c);
        f[chull(f),]
      }
    )
  )
}
hclust.hull <- find_convex_hull(hclust.project2D, groups)
```

```{r, fig.width=8, fig.height=4}
ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
geom_point(aes(shape=cluster, color=cluster)) +
geom_text(aes(label=country, color=cluster), hjust=0, vjust=1, size=3) +
geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),
alpha=0.4, linetype=0) + theme(text=element_text(size=8))

```

When visualizing the clusters using the first 2 principal components, we notice that there are 2 distinct clusters, rather than the 3 clusters found in the `ward.D2` dendrogram. This suggests that clusters 2 and 3 in the dendrogram are not very well seperated which is why they appear as one cluster in this 2 dimensional PCA plot. 

## Using clusterboot() to assess stability

```{r, results='hide'}
kbest.p <- 3
cboot.hclust <- clusterboot(youtube_clustering, clustermethod=hclustCBI,
method="ward.D2", k=kbest.p)

```

```{r}
summary(cboot.hclust$result)

```
```{r}
groups.cboot <- cboot.hclust$result$partition
values <- 1 - cboot.hclust$bootbrd/100
```

## Finding the value of k

```{r}
# Function to return the squared Euclidean distance of two given points x and y
sqr_euDist <- function(x, y) {
  sum((x - y)^2)
}

# Function to calculate WSS of a cluster, represented as a n-by-d matrix
# (where n and d are the numbers of rows and columns of the matrix)
# which contains only points of the cluster.
wss <- function(clustermat) {
  c0 <- colMeans(clustermat)
  sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}

# Function to calculate the total WSS. Argument `scaled_df`: data frame
# with normalised numerical columns. Argument `labels`: vector containing
# the cluster ID (starting at 1) for each row of the data frame.
wss_total <- function(scaled_df, labels) {
  wss.sum <- 0
  k <- length(unique(labels))
  for (i in 1:k)
      wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
  wss.sum
}

# Function to calculate total sum of squared (TSS) distance of data
# points about the (global) mean. This is the same as WSS when the
# number of clusters (k) is 1.
tss <- function(scaled_df) {
  wss(scaled_df)
}

# Function to return the CH indices computed using hierarchical
# clustering (function `hclust`) or k-means clustering (`kmeans`)
# for a vector of k values ranging from 1 to kmax.
CH_index <- function(scaled_df, kmax, method="kmeans") {
  if (!(method %in% c("kmeans", "hclust")))
    stop("method must be one of c('kmeans', 'hclust')")
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
  # wss.value[1] stores the WSS value for k=1 (when all the
  # data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  
  if (method == "kmeans") {
    # kmeans
    for (k in 2:kmax) {
    clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
    wss.value[k] <- clustering$tot.withinss
    }
  } else {
    # hclust
    d <- dist(scaled_df, method="euclidean")
    pfit <- hclust(d, method="ward.D2")
    for (k in 2:kmax) {
      labels <- cutree(pfit, k=k)
      wss.value[k] <- wss_total(scaled_df, labels)
    }
  }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax-1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
```

```{r, fig.width=10, fig.height=4}
# calculate the CH criterion
crit.df <- CH_index(youtube_clustering, 10, method="hclust")

fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
  geom_point() + geom_line(colour="red") +
  scale_x_continuous(breaks=1:10, labels=1:10) +
  labs(y="CH index") + theme(text=element_text(size=8))

fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
  geom_point() + geom_line(colour="blue") +
  scale_x_continuous(breaks=1:10, labels=1:10) +
  theme(text=element_text(size=8))

grid.arrange(fig1, fig2, nrow=1)
```

Using the CH Index and WSS to find the best value of K, where the higher the value the better. Using this method, we find that K=2 has the highest scores for both CH Index and WSS. This supports what we found earlier in the PCA plot where there were 2 clusters instead of 3. 





















