---
title: "CITS4009 - Project 2"
author: "Joo Kai Tay (22489437)"
date: "2023-09-25"
output: html_document
---
```{css}
img {
  display: block;
  margin-bottom: 0 !important;
}
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

- Introduce the youtube dataset and the features

- Start by talking about some EDA stuff 
  - Add graphs and talk about missing data and w/e
  - Histograms
  - Do log transforms of data
  - Explore which variables need to be scaled 
  - Binarize the target variable
    - Try mean / median / other sensible values 
  
- Basic data cleaning
  - Discard character string and categorical columns
  - Convert the factors 
  
- Choose the response variable for the task
  - Formulate it as a binary classification problem 
  
- Split the data into test and train set 
  - Justify this

- Implement a NULL model

- Implement a single-variable model 
  - Try every single variable and pick out the best features
  - Use this as a baseline to compare with the complex models

- Implement decision tree classifier

- Implement the following
  - logistic regression
  - naive bayes 
  - KNN
  - FIND THE MOST INTERESTING ONE AND DO SOMEHING WITH IT
  - Attribute & feature selection 
    - Do a load with this (like a lot, lots and lots)
    - 3-4 attribute selection techniques 
    - Mix and match 
    - and compare the results of all the different types and make pretty graphs for it
    - end myself
  
- Things to investigate
  - Earnings
  - Subscribers
  - 

# Introduction

The 2023 Global YouTube Statistic dataset can be accessed from Kaggle from the link below:
https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023

This dataset contains various information about the top YouTube channels on the platform with the most subscribers. The data set contains information about their subscriber counts, video views, earnings and demographics.

The modelling process explored to preduct **REPLACE ONCE WE KNOW WHAT TO PREDICT** is documented below in terms of code and comments. The plots 

# Data loading, and set up

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(knitr)
library(tidyverse)
library(ggrepel)
library(vtreat)
library(corrplot)
library(tidyr)
library(psych)
library(GGally) 
library(stringr)
library(reshape2)
library(caret)
library(ROCR)
```

```{r}
path <- './data/youtube_UTF_8.csv'
youtube <- read.csv(path)
```

# Functions

This section contains all function used in this modelling exercise. 

```{r}
# This function plots a correlation matrix for each numerical value in a given a dataframe
# @param df dataframe to plot
plot_corr <- function(df) {
  youtube_numeric <- df[, sapply(df, is.numeric) | sapply(df, is.integer)]
  
  correlation_matrix <- cor(youtube_numeric, use = "complete.obs")
  correlation_data <- melt(correlation_matrix)
  diag(correlation_matrix) <- NA
  
  format_label <- function(x) {
    ifelse(is.na(x), "", ifelse(abs(x) >= 0.6,sprintf("%.1f", x), ""))
  }
  
  custom_color_gradient <- c('forestgreen', 'lightblue', 'darkblue')
  
  heatmap_plot <- ggplot(correlation_data, aes(Var1, Var2, fill = value)) +
    geom_tile() +
    geom_text(aes(label = format_label(value)), vjust = 0.5, hjust=0.5, size = 2, color='white') +  
    scale_fill_gradientn(colors = custom_color_gradient, values = scales::rescale(c(-1, 0, 1)), guide = "legend") +  
      labs(
      title = "Correlation Matrix",
      x = NULL,
      y = NULL
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      plot.title = element_text(hjust = 0.5))
    
  
  print(heatmap_plot)
}
```

```{r}
# This function plots a histogram for each numerical value in a given dataframe
# @param df dataframe to plot
# @param ncols number of columns in output plot
# @param var.type which selection of variables to plot
plot_hist <- function(df, ncols, var.type){
  
  numeric_cols <- sapply(df, is.numeric)  
  if(var.type == "log"){
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.log$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  } else if(var.type == "default") {
    total_numeric_cols <- sum(numeric_cols) 
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  } else if(var.type == "minmax") {
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.minmax$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))
  } else if(var.type == "norm") {
    total_numeric_cols <- sum(sapply(df, is.numeric) & grepl("\\.norm$", names(df)))
    nrows <- ceiling(total_numeric_cols/ncols) 
    par(mfrow=c(nrows,ncols), mar=c(2, 2, 1, 1))  
  }
  
  
  for(colname in names(df)[numeric_cols]) {
    if(var.type == "log" & grepl("\\.log$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if(var.type == "default") {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if (var.type == "minmax" & grepl("\\.minmax$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    } else if(var.type == "norm" & grepl("\\.norm$", colname)) {
      hist(df[[colname]], main=colname, xlab=NULL, col="lightblue", border="black", breaks=50)
    }
  }
  
  # Reset the plotting parameters to default
  par(mfrow=c(1, 1), mar=c(5, 4, 4, 2) + 0.1)
}

```

```{r}
plot_individual_hist <- function(df, feature, title1, plot.mean) {
  mean_val <- mean(df[,feature], na.rm = TRUE)
  median_val <- median(df[,feature], na.rm = TRUE)
  
  p <- ggplot(df, aes(x = df[,feature])) +
  geom_histogram(fill = "lightgreen", alpha = 0.7, color="black") +
  labs(title=title1) +
  xlab(feature) +
  ylab("Frequency") +
  theme_minimal()
  
  if(plot.mean) {
    p <- p +
      geom_vline(aes(xintercept = mean_val), color = "red", linetype = "dashed", size = 1) +
      geom_vline(aes(xintercept = median_val), color = "blue", linetype = "dashed", size = 1) +
      annotate("text", x = mean_val, y = Inf, label = sprintf("Mean = %.2f", mean_val), vjust = 2, hjust = 1.1, color = "red", size = 3.5) +
      annotate("text", x = median_val, y = Inf, label = sprintf("Median = %.2f", median_val), vjust = 2, hjust = -0.1, color = "blue", size = 3.5) 
  }
  
  return(p)
}
```


```{r}
minmax_scale <- function(x) {
  return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
# This function transforms the data 
# @param df the dataframe to transform
# @param features the features to transform
# @param trans the type of transform to apply
transform_cols <- function(df, features, trans) {
  if(trans == "log"){
    # Uses log1p which computes log(1 + x) to avoid returning NaN on 0 or -ve values
    df <- df %>%
      mutate(across(all_of(features), log1p, .names = "{col}.log"))
  } else if (trans == "minmax") {
    # Applying min-max scaling
    df <- df %>%
      mutate(across(all_of(features), minmax_scale, .names = "{col}.minmax"))
  } else if (trans == "norm") {
    # Applying z-normalization
    df <- df %>%
      mutate(across(all_of(features), scale, .names = "{col}.norm"))
  }
  
  return(df)
}
```

```{r}
# This function implements a NULL model for a given set of data 
# @param features The feature columns
# @param target The target variable
```

# Data Preparation

```{r, results='hide'}
str(youtube)
```
The Global YouTube Statistics 2023 data set contains 995 observations for 28 variables:

- 4 Integer Variables: Integers represent whole numbers without decimal points
- 7 Character Variables: Characters represent text or strings of characters
- 17 Numeric Variables: Represent real numbers, including integers & numbers with decimal points

## Unique Observations 

```{r}
unique_counts <- sapply(youtube, function(x) length(unique(x)))
unique_counts_df <- data.frame(Column_Name = names(unique_counts), Unique_Count = unique_counts)
print(unique_counts_df)
```

The code above counts the number of unique rows in each feature of the data set. It can be observed that the features `rank`, `Youtuber` and `Title` have unique values in each row. When a feature has a unique value for each row, it means there is no discernible pattern or variation in that feature. Machine learning models rely on patterns and relationships in the data to make predictions. Therefore, including features that cannot contribute to learning these patterns would just introduce noise into any model that we attempt to fit to them. For this reason, these features will be dropped from the data set. 

```{r}
youtube <- subset(youtube, select = -c(rank, Youtuber, Title))
```

## Data Correlation Matrix 

```{r,fig.dim = c(15, 15)}
plot_corr(youtube)
```

When investigating if some features can be dropped due to a linear relationship, it is common to plot a scatter matrix that plots each feature against each other feature. However, in this dataset which contains 28 features, that would require 28^2 (784) plots. This is an unrealistic number to plot, display and sort through.

Therefore, we can plot a correlation matrix. The correlation value between a pair of features measures the degree of the linear relationship that they have. If the two features have a high correlation score (approaching 1) it means that they are linear and measure the same variable. Having too many highly correlated variables may present problems in the following ways:

- **Over fitting**: The model may make splits based on a small number of highly correlated features. This may cause over fitting problems.
- **Redundancy**: As seen in section 1.2.1 removing highly correlated features only gives a small drop in accuracy while providing a significant increase in performance. This is because highly correlated features provide very little additional benefits in classification while adding unecessary complexity.
- **Improved Model Performance**: In some cases, removing highly correlated features can actually improve the performance of the model. When features are highly correlated, they can lead to multicollinearity, which can make it difficult for the model to estimate the individual effects of each feature on the target variable.

Examining the plot above, we can observe the following:

- `Urban_population` is highly correlated with `Population`
- `video_views_rank`, `country_rank` and `channel_type_rank` are highly correlated
- `video.views` and `subscribers` are highly correlated

We will drop `Urban_population`, `country_rank` and `channel_type_rank` from the data set.

```{r}
youtube <- subset(youtube, select=-c(`Urban_population`, `country_rank`, `channel_type_rank`))
```

## Data Cleaning

The following steps were discussed in project 1 in relation to the data cleaning and will be repeated here:

There are channels with 0 uploads or video views. We are not interested in these channels as the data is likely to be incorrect given their popularity. Therefore, all observations with video views or uploads equal to 0 will be replaced with NaN.

```{r}
youtube <- youtube %>%
  mutate(video.views = na_if(video.views, 0),
         uploads = na_if(uploads, 0))
```

YouTube was founded in 2005, therefore any observation with a channel creation year prior to 2005 are errors and will be replaced with NaN to be counted as missing values.

```{r}
youtube <- youtube %>%
  mutate(created_year = ifelse(created_year < 2005, NA, created_year))
```

Data points in which the missing data was indicated using `nan` instead of `NA` which is the proper value to represent missing data in R. We will convert all those values to `NaN` in order to have all missing values represented uniformly.

```{r}
youtube <- youtube %>%
  mutate_all(~ ifelse(. %in% c("nan"), NA, .))
```

The rows where `video.views` and `uploads` are missing are also missing most of the data regarding earnings and geographical data. Therefore, we consider these rows to be of low quality and they will be deleted from the data set. 

```{r}
youtube <- youtube[!(is.na(youtube$video.views) | is.na(youtube$uploads)), ]
```

For categorical variables, any values with NA or NaN will be converted to `missing`.

```{r}
youtube <- youtube %>%
  mutate(category = ifelse(is.na(category), "missing", category),
         Country = ifelse(is.na(Country), "missing", Country),
         Abbreviation = ifelse(is.na(Abbreviation), "missing", Abbreviation),
         channel_type = ifelse(is.na(channel_type), "missing", channel_type),
         created_month = ifelse(is.na(created_month), "missing", created_month))
```

The following columns contain duplicate information and will be removed:
```{r}
youtube <- subset(youtube, select = -c(category, Abbreviation))
```

## Histograms of numeric variables

```{r, fig.dim = c(10, 10)}
plot_hist(youtube, 3, "default")
```

The function above shows the plot of histograms for the remaining numeric variables in the data set. It can be observed that many of the features, including `subscribers`, `video.views`, `uploads`, `video_views_rank`, `video_views_for_the_last_30_days`, `lowest_monthly_earnings`, `highest_monthly_earnings`, `lowest_yearly_earnings`, `highest_yearly earnings` and `subscribers_for_last_30_days` are right skewed. This means that the majority of the data points are concentrated on the left side of the distribution, with the tail extending to the right. In such cases, the mean is typically greater than the median, and extreme values can be substantially higher than the rest of the data points.    

Having a right skew can impact model performance as many machine learning models assume that the features have a normal (Gaussian) distribution. When the features are skewed, it can lead to sub optimal performance as the models are less capable of capturing the underlying patterns in the skewed data.

We will investigate several transformations of these variables to determine which is the best option for our data. We will apply a log transform, min-max scaling and z-normalization to the following columns:

```{r}
cols_to_transform <- c(
  "subscribers", "video.views", "uploads", "video_views_rank", 
  "video_views_for_the_last_30_days", "lowest_monthly_earnings", 
  "highest_monthly_earnings", "lowest_yearly_earnings", 
  "highest_yearly_earnings", "subscribers_for_last_30_days"
)
```


### Log transform

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, colnames(youtube)[sapply(youtube, is.numeric)],"log")

plot_hist(youtube, 3, "log")
```

### Min-Max scaling

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, cols_to_transform,"minmax")
plot_hist(youtube, 3, "minmax")
```

The min-max scaler transforms the data to be within a given range which is very useful when dealing with data that has vastly different scales like the YouTube data set. In this case, the range chosen was [0,1]. However, just like z-normalization, min-max scaling is highly sensitive to outliers. Given the highly skewed distribution with a very long tail, min-max scaling will compress the majority of the scaled data into a small interval.

### Z-score normalization

```{r, fig.width=10, fig.height=7}
youtube <- transform_cols(youtube, cols_to_transform,"norm")
plot_hist(youtube, 3, "norm")
```

Z-normalization involves scaling the features such that they have a mean of zero and a standard deviation of one. However, as shown in the plots above Z-normalization is not very useful for the YouTube data set. This is because Z-normalization is sensitive to outliers. Given that many of the features are heavily skewed with extreme values, the mean and standard deviation can be disproportionately affected. Standardizing in this context may not provide the intended scaling for the majority of the data points. Therefore, using Z-normalization does not address the issue of the skewed data that we are trying to rectify. 

# Target Variable Selection 

Given the nature of the YouTube data set, we will choose earnings as the target variable to determine the success or failure of a YouTube channel. To facilitate the classification, we will formulate it as a binary classification problem (high-earning/low-earning) as multiclass classification is very difficult. 

```{r, fig.width=10, fig.height=4}
earnings_subset <- youtube[, c("lowest_monthly_earnings", 
  "highest_monthly_earnings", "lowest_yearly_earnings", 
  "highest_yearly_earnings")]

ggpairs(earnings_subset) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10), 
        axis.text.y = element_text(size = 10))
```

The pairplot above shows the 4 different features that track the earnings of each YouTube channel. It can be observed that there ia a perfect positive linear relationship in each case and they all have a correlation of 1. It's worth noting that in real-world scenarios, having a correlation of exactly 1 between different variables is very rare, unless there is some deterministic relationship between them or they are different representations of the same underlying data.

We will take an average of the highest and lowest yearly earnings to form a new feature that we will use as our target variable. There are several reasons to use the average of the highest and lowest yearly earnings:

- Predicting the highest earnings may lead to over-optimistic predictions, influenced by extreme values or outliers. An average helps in reducing the influence of such extreme values.
- High earnings may have high variability, and focusing solely on them may lead to instability in predictions. Averaging helps in mitigating this variability.
- A model trained to predict average earnings tends to be more robust, as it learns the broader patterns in the data.

```{r, fig.width=10, fig.height=4}
youtube <- youtube %>%
  mutate(average_yearly_earnings = (lowest_yearly_earnings + highest_yearly_earnings) / 2)

youtube <- transform_cols(youtube, "average_yearly_earnings", "log")

p1 <- plot_individual_hist(youtube, "average_yearly_earnings", "Average Yearly Earnings", TRUE)
p2 <- plot_individual_hist(youtube, "average_yearly_earnings.log", "Log of Average Yearly Earnings", TRUE)

grid.arrange(p1,p2, ncol=2)
```


The histogram on the left shows the average yearly earnings target variable that was just created. Like many of the other features in this data set, it is right skewed. Therefore, we will perform a log transform on it to help normalize the distribution of the target variable. 

As the earnings is presented as a continuous variable, we will need to find a threshold to determine what is considered high and low earning in order to formulate this as a binary classification problem. We will examine using the mean and median as the initial threshold.


```{r}
youtube$average_yearly_earnings.binary <- ifelse(youtube$average_yearly_earnings.log > 12.15, 1, 0)
table(youtube$average_yearly_earnings.binary)
```
```{r}
youtube$average_yearly_earnings.binary <- ifelse(youtube$average_yearly_earnings.log > 14.14, 1, 0)
table(youtube$average_yearly_earnings.binary)
```

The code above shows the number of each class in the newly created binary target variable when using the mean and median as the splitting threshold respectively. We can see that using the median produces a much better balance between the positive and negative classes this is important as the balance between positive and negative classes in the target variable can significantly impact the performance and interpretation of a machine learning model. When there's a strong class imbalance, the majority class can dominate the minority class, leading to poor predictive performance for the minority class. A model may achieve high overall accuracy simply by predicting the majority class all the time, but it's likely that the recall or precision for the minority class will be very low.

Therefore, going forward we will use the median to split the target variable for the binary classification tasks in the remainder of this notebook.

```{r}
outcome <- "average_yearly_earnings.binary"
pos <- 1
```


# Splitting the data

We will use a 90/10 split for the data. Given that the data set is relatively small (1000 obvs), we allocate a larger proportion for training to ensure that the model has enough data to learn for. 

```{r}
# This function splits a dataframe into training and test sets given a particular ratio
# @param df the dataframe to operate on
# @param feature the feature to use as the vector of outcomes
# @param train_ratio the split to use
split_data <- function(df, feature, train_ratio = 0.9) {
  train_indices <- createDataPartition(df[,feature],  p= train_ratio, list = FALSE)
  
  # Split the data into training and test sets
  train_set <- df[train_indices, ]
  test_set <- df[-train_indices, ]
  
  # Return a list containing the training and test sets
  list(train = train_set, test = test_set)
}


```
```{r}
# Retain only the log transformed variables
youtube_cleaned <- youtube[, grepl("\\.log$", names(youtube))]
youtube_cleaned <- bind_cols(youtube_cleaned, youtube[, c("average_yearly_earnings.binary", "Country", "channel_type", "created_month")])

splits <- split_data(youtube_cleaned, "average_yearly_earnings.binary", train_ratio = 0.9)
train_data <- splits$train
test_data <- splits$test

useForCal <- rbinom(n=dim(train_data)[[1]], size=1, prob=0.1) > 0
cal_data <- subset(train_data, useForCal)
train_data <- subset(train_data,!useForCal)
```

Split the data into categorical and numerical variables:

```{r}
# This function seperates a dataframe into categorical and numeric variables
# @param df the dataframe to operate on
separate_vars <- function(df) {
  numeric_var_names <- names(df)[sapply(df, is.numeric)]
  factor_var_names <- names(df)[sapply(df, is.character)]

  numVars <- df %>%
    select(all_of(numeric_var_names))

  catVars <- df %>%
    select(all_of(factor_var_names))
  
  return(list(numVars = numVars, catVars = catVars))
}

```

```{r}
sep_train <- separate_vars(train_data)
numVars <- sep_train$numVars
catVars <- sep_train$catVars
```


# Single-Variable Models

The following function is used to process the categorical variables and perform single variable prediction for a given categorical column.

```{r}
mkPredC <- function(outCol, varCol, appCol) {
  pPos <- sum(outCol==pos)/length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab/sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}

for (v in colnames(catVars)) {
  pi <- paste('pred', v, sep='')
  train_data[,pi] <- mkPredC(train_data[,outcome], train_data[,v], train_data[,v])
  cal_data[,pi] <- mkPredC(cal_data[,outcome], cal_data[,v], cal_data[,v])
  test_data[,pi] <- mkPredC(test_data[,outcome], test_data[,v], test_data[,v])
}
```

The following function is used to process the numeric variables and perform single variable prediction for a given numeric column.

```{r}
# for numerical variables, we convert them into categorical one and
# call the `mkPredC` function above.
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(as.numeric(
    quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T)))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}

# now go through all the numerical variables in the `numericVars` vector
# and perform the predictions. Again, the outputs are stored back into
# the data frame.
for (v in colnames(numVars)) {
  pi <- paste('pred', v, sep='')
  train_data[,pi] <- mkPredN(train_data[,outcome], train_data[,v], train_data[,v])
  test_data[,pi] <- mkPredN(test_data[,outcome], test_data[,v], test_data[,v])
  cal_data[,pi] <- mkPredN(cal_data[,outcome], cal_data[,v], cal_data[,v])
}
```

## Evaulating Single Variable Models using AUC

```{r}
calcAUC <- function(predcol, outcol) {
  perf <- performance(prediction(predcol, outcol==pos),'auc')
  as.numeric(perf@y.values)
}
```

```{r, message=TRUE}
for(v in colnames(catVars)) {
  pii <- paste('pred',v,sep='')
  aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])

  aucCal <- calcAUC(cal_data[,pii],cal_data[,outcome])
  print(sprintf(
    "%s, trainAUC: %4.3f calibrationAUC: %4.3f",
    pii, aucTrain, aucCal))
  
}
```
```{r}
for(v in colnames(numVars)) {
  pii<-paste('pred',v,sep='')
  aucTrain <- calcAUC(train_data[,pii],train_data[,outcome])
  
  if(aucTrain>=0.3) {
    aucCal<-calcAUC(cal_data[,pii],cal_data[,outcome])
    print(sprintf(
      "%s, trainAUC: %4.3f calibrationAUC: %4.3f",
      pii,aucTrain,aucCal))
  }
}
```
```{r, fig.height=4}
ggplot(data=cal_data) +
  geom_density(aes(x=uploads.log,color=as.factor(average_yearly_earnings.binary)))
```

## Evaulating Single Variable Models using Log likelihood

```{r}
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1-ypred)), na.rm=T)
}
```

Compute the log likelihood of the Null model

```{r}
# Compute the likelihood of the Null model on the calibration set
logNull <- logLikelihood(sum(cal_data[,outcome]==pos)/nrow(cal_data), cal_data[,outcome]==pos)

cat("The log likelihood of the Null model is:", logNull)
```
Run through categorical variables to select top performers

```{r}
# selCatVars is a vector that keeps the names of the top performing categorical variables.
selCatVars <- c()
minDrop <- 0  # may need to adjust this number

for (v in colnames(catVars)) {
  pi <- paste('pred', v, sep='')
  devDrop <- 2*(logLikelihood(cal_data[,pi], cal_data[,outcome]==pos) - logNull)
  if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selCatVars <- c(selCatVars, pi)
  }
}
selCatVars
```
Run through numerical variables to select top performers

```{r}
selNumVars <- c()
minDrop <- -10  # may need to adjust this number
for (v in colnames(numVars)) {
  pi <- paste('pred', v, sep='')
  devDrop <- 2*(logLikelihood(cal_data[,pi], cal_data[,outcome]==pos) - logNull)
  print(devDrop)
  if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selNumVars <- c(selNumVars, pi)
  }
}
selNumVars
```


# Multi-Variable Models

- copy from machine learning






